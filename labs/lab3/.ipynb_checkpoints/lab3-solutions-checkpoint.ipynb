{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNLP: Lab Session 3\n",
    "\n",
    "# Hidden Markov Models - Construction and Use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages used for this lab\n",
    "\n",
    "import nltk\n",
    "\n",
    "# import brown corpus\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# module for training a Hidden Markov Model and tagging sequences\n",
    "from nltk.tag.hmm import HiddenMarkovModelTagger\n",
    "\n",
    "# module for computing a Conditional Frequency Distribution\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "# module for computing a Conditional Probability Distribution\n",
    "from nltk.probability import ConditionalProbDist\n",
    "\n",
    "# module for computing a probability distribution with the Maximum Likelihood Estimate\n",
    "from nltk.probability import MLEProbDist\n",
    "\n",
    "# pretty printing\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Corpora tagged with Part-of-Speech information\n",
    "\n",
    "NLTK provides corpora annotated with Part-of-Speech (POS) information and\n",
    "some tools to access this information. The Penn Treebank tagset is commonly\n",
    "used for annotating English sentences. We can inspect this tagset in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown corpus provided with NLTK is also tagged with POS information,\n",
    "although the tagset is slightly different than the Penn Treebank tagset. Information about the Brown corpus tagset can be found here:\n",
    "http://www.scs.leeds.ac.uk/ccalas/tagsets/brown.html\n",
    "\n",
    "We can retrieve the tagged sentences in the Brown corpus by calling the `tagged_sents()`\n",
    "function and looking at an annotated sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tagged with Penn Treebank POS labels:\n",
      "[   ('The', 'AT'),\n",
      "    ('jury', 'NN'),\n",
      "    ('praised', 'VBD'),\n",
      "    ('the', 'AT'),\n",
      "    ('administration', 'NN'),\n",
      "    ('and', 'CC'),\n",
      "    ('operation', 'NN'),\n",
      "    ('of', 'IN'),\n",
      "    ('the', 'AT'),\n",
      "    ('Atlanta', 'NP-TL'),\n",
      "    ('Police', 'NNS-TL'),\n",
      "    ('Department', 'NN-TL'),\n",
      "    (',', ','),\n",
      "    ('the', 'AT'),\n",
      "    ('Fulton', 'NP-TL'),\n",
      "    ('Tax', 'NN-TL'),\n",
      "    (\"Commissioner's\", 'NN$-TL'),\n",
      "    ('Office', 'NN-TL'),\n",
      "    (',', ','),\n",
      "    ('the', 'AT'),\n",
      "    ('Bellwood', 'NP'),\n",
      "    ('and', 'CC'),\n",
      "    ('Alpharetta', 'NP'),\n",
      "    ('prison', 'NN'),\n",
      "    ('farms', 'NNS'),\n",
      "    (',', ','),\n",
      "    ('Grady', 'NP-TL'),\n",
      "    ('Hospital', 'NN-TL'),\n",
      "    ('and', 'CC'),\n",
      "    ('the', 'AT'),\n",
      "    ('Fulton', 'NP-TL'),\n",
      "    ('Health', 'NN-TL'),\n",
      "    ('Department', 'NN-TL'),\n",
      "    ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences = brown.tagged_sents(categories='news')\n",
    "print('Sentence tagged with Penn Treebank POS labels:')\n",
    "pp.pprint(tagged_sentences[29])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is useful to use a coarser label set in order to avoid data sparsity\n",
    "or to allow a mapping between the POS labels for different languages. The Universal tagset was designed to be applicable for all languages:\n",
    "\n",
    "https://github.com/slavpetrov/universal-pos-tags\n",
    "\n",
    "There are mappings between the POS tagset of several languages and the Universal tagset. We can access the Universal tags for the Brown corpus sentences\n",
    "by changing the tagset argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tagged with Universal POS:\n",
      "[   ('The', 'DET'),\n",
      "    ('jury', 'NOUN'),\n",
      "    ('praised', 'VERB'),\n",
      "    ('the', 'DET'),\n",
      "    ('administration', 'NOUN'),\n",
      "    ('and', 'CONJ'),\n",
      "    ('operation', 'NOUN'),\n",
      "    ('of', 'ADP'),\n",
      "    ('the', 'DET'),\n",
      "    ('Atlanta', 'NOUN'),\n",
      "    ('Police', 'NOUN'),\n",
      "    ('Department', 'NOUN'),\n",
      "    (',', '.'),\n",
      "    ('the', 'DET'),\n",
      "    ('Fulton', 'NOUN'),\n",
      "    ('Tax', 'NOUN'),\n",
      "    (\"Commissioner's\", 'NOUN'),\n",
      "    ('Office', 'NOUN'),\n",
      "    (',', '.'),\n",
      "    ('the', 'DET'),\n",
      "    ('Bellwood', 'NOUN'),\n",
      "    ('and', 'CONJ'),\n",
      "    ('Alpharetta', 'NOUN'),\n",
      "    ('prison', 'NOUN'),\n",
      "    ('farms', 'NOUN'),\n",
      "    (',', '.'),\n",
      "    ('Grady', 'NOUN'),\n",
      "    ('Hospital', 'NOUN'),\n",
      "    ('and', 'CONJ'),\n",
      "    ('the', 'DET'),\n",
      "    ('Fulton', 'NOUN'),\n",
      "    ('Health', 'NOUN'),\n",
      "    ('Department', 'NOUN'),\n",
      "    ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences_universal = brown.tagged_sents(categories='news', tagset='universal')\n",
    "print('Sentence tagged with Universal POS:')\n",
    "pp.pprint(tagged_sentences_universal[29])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initial universal tagset was later expanded as part of the Universal Dependencies project. The resulting tagset is called UPOS and you can find more information in the link below. This tagset is not yet supported by NLTK. However, it is important that you know about it since it is the most used multi-lingual tagset nowadays.\n",
    "\n",
    "https://universaldependencies.org/u/pos/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Exploring Brown corpus\n",
    "\n",
    "In this exercise we will explore the Brown corpus, specifically its frequency distribution over POS tags.\n",
    "The Brown corpus is divided in topical categories called 'genres'. Let's see what genres we have in the corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   'adventure',\n",
      "    'belles_lettres',\n",
      "    'editorial',\n",
      "    'fiction',\n",
      "    'government',\n",
      "    'hobbies',\n",
      "    'humor',\n",
      "    'learned',\n",
      "    'lore',\n",
      "    'mystery',\n",
      "    'news',\n",
      "    'religion',\n",
      "    'reviews',\n",
      "    'romance',\n",
      "    'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(brown.categories())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You task in this exercise is to implement a function that computes the Frequency Distribution over a Brown genre category and a tagset scheme.\n",
    "The template of the function is given below. It takes two parameters: one is the genre category and the other is the tagset name.\n",
    "Your job is to do the following:\n",
    "\n",
    "1. Convert the list of (word,tag) pairs to a list of tags\n",
    "2. Use the list of tags to compute a frequency distribution over the tags. Use NLTK's `FreqDist()`\n",
    "3. Compute the total number of tags in the Frequency Distribution\n",
    "4. Return the total number of tags and the top 10 most frequent tags\n",
    "\n",
    "You are given the code to retrieve the list of (word, tag) tuples from the brown corpus corresponding to the given category and tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_tagset_frequency_distro(genre, tagset):\n",
    "    \"\"\"Compute a Frequency distribution of the POS tags in a genre of the tagged Brown corpus\n",
    "    \n",
    "    :param genre: A Brown corpus genre\n",
    "    :type genre: str or iterable(str) or None\n",
    "    :param tagset: A Brown tagset name\n",
    "    :type tagset: str or None (defaults to 'brown')\n",
    "    :return: number of tag types, top 10 tags\n",
    "    :rtype: tuple(int,list(tuple(str,int))\"\"\"\n",
    "\n",
    "    # get the tagged words from the corpus\n",
    "    tagged_words = brown.tagged_words(categories=genre, tagset=tagset)\n",
    "\n",
    "    # TODO: convert tagged_words to a list of tags\n",
    "    tags = ( tp[1] for tp in tagged_words)\n",
    "\n",
    "    # TODO: using the above list compute a Frequency Distribution\n",
    "    # hint: use nltk.FreqDist()\n",
    "    tagsFDist = nltk.FreqDist(tags)\n",
    "\n",
    "    # TODO: retrieve the number of tag types in the tagset\n",
    "    # hint: help(nltk.FreqDist)\n",
    "    number_of_tags = tagsFDist.B()\n",
    "\n",
    "    # TODO: retrieve the top 10 most frequent tags and their counts\n",
    "    top_tags = tagsFDist.most_common(10)\n",
    "\n",
    "    return number_of_tags, top_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code with this function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag FreqDist for news with Penn Treebank tagset:\n",
      "(   218,\n",
      "    [   ('NN', 13162),\n",
      "        ('IN', 10616),\n",
      "        ('AT', 8893),\n",
      "        ('NP', 6866),\n",
      "        (',', 5133),\n",
      "        ('NNS', 5066),\n",
      "        ('.', 4452),\n",
      "        ('JJ', 4392),\n",
      "        ('CC', 2664),\n",
      "        ('VBD', 2524)])\n",
      "Tag FreqDist for science_fiction with Penn Treebank tagset:\n",
      "(   127,\n",
      "    [   ('NN', 1541),\n",
      "        ('IN', 1176),\n",
      "        ('.', 1077),\n",
      "        ('AT', 1040),\n",
      "        (',', 791),\n",
      "        ('JJ', 723),\n",
      "        ('NNS', 532),\n",
      "        ('VBD', 531),\n",
      "        ('RB', 522),\n",
      "        ('VB', 495)])\n",
      "Tag FreqDist for news with Universal tagset:\n",
      "(   12,\n",
      "    [   ('NOUN', 30640),\n",
      "        ('VERB', 14399),\n",
      "        ('ADP', 12355),\n",
      "        ('.', 11928),\n",
      "        ('DET', 11389),\n",
      "        ('ADJ', 6706),\n",
      "        ('ADV', 3349),\n",
      "        ('CONJ', 2717),\n",
      "        ('PRON', 2535),\n",
      "        ('PRT', 2264)])\n",
      "Tag FreqDist for science_fiction with Universal tagset:\n",
      "(   12,\n",
      "    [   ('NOUN', 2747),\n",
      "        ('VERB', 2579),\n",
      "        ('.', 2428),\n",
      "        ('DET', 1582),\n",
      "        ('ADP', 1451),\n",
      "        ('PRON', 934),\n",
      "        ('ADJ', 929),\n",
      "        ('ADV', 828),\n",
      "        ('PRT', 483),\n",
      "        ('CONJ', 416)])\n"
     ]
    }
   ],
   "source": [
    "def test_ex1():\n",
    "    print('Tag FreqDist for news with Penn Treebank tagset:')\n",
    "    pp.pprint(explore_tagset_frequency_distro('news', None))\n",
    "\n",
    "    print('Tag FreqDist for science_fiction with Penn Treebank tagset:')\n",
    "    pp.pprint(explore_tagset_frequency_distro('science_fiction', None))\n",
    "\n",
    "    # Do the same thing for a different tagset: Universal\n",
    "\n",
    "    print('Tag FreqDist for news with Universal tagset:')\n",
    "    pp.pprint(explore_tagset_frequency_distro('news', 'universal'))\n",
    "\n",
    "    print('Tag FreqDist for science_fiction with Universal tagset:')\n",
    "    pp.pprint(explore_tagset_frequency_distro('science_fiction', 'universal'))\n",
    "\n",
    "test_ex1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the top tags for different genre and tagsets. Observe differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating an HMM Tagger\n",
    "\n",
    "NLTK provides a module for training a Hidden Markov Model for sequence tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(nltk.tag.hmm.HiddenMarkovModelTagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train the HMM for POS tagging given a labelled dataset. At the begging of this lab we learned how to access the labelled sentences of the Brown corpus.\n",
    "We will use this dataset to study the effect of the size of the training corpus on\n",
    "the accuracy of the tagger.\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "In this exercise we will train a HMM tagger on a training set and evaluate it\n",
    "on a test set. The template of the function that you have to implement takes\n",
    "two parameters: a sentence to be tagged and the size of the training corpus in\n",
    "number of sentences. You are given the code that creates the training and test\n",
    "datasets from the tagged sentences in the Brown corpus.\n",
    "\n",
    "1. Train a Hidden Markov Model tagger on the training dataset. Refer to `help(nltk.tag.hmm.HiddenMarkovModelTagger.train)` if necessary.\n",
    "2. Use the trained model to tag the sentence\n",
    "3. Use the trained model to evaluate the tagger on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_trainer(sentence, size):\n",
    "    \"\"\" Create an HMM tagger from the Brown news corpus, and test it by tagging a sample sentence\n",
    "    \n",
    "    :param sentence: An untagged sentence as an example\n",
    "    :type sentence: list(str)\n",
    "    :param size: Number of sentences to train on (be sure to leave room for the test data)\n",
    "    :type size: int\n",
    "    :return: The tagger, the sample sentence with tags, entropy of model wrt 100 test sentences\n",
    "    :rtype: tuple(nltk.tag.hmm.HiddenMarkovModelTagger, list(tuple(str,str)), float)\"\"\"\n",
    "    tagged_sentences = brown.tagged_sents(categories='news')\n",
    "\n",
    "    # set up the training data\n",
    "    train_data = tagged_sentences[-size:]\n",
    "\n",
    "    # set up the test data\n",
    "    test_data = tagged_sentences[:100]\n",
    "    \n",
    "    # Hint: use help on HiddenMarkovModelTagger to find out how to train, tag and evaluate an HMM tagger\n",
    "\n",
    "    # TODO: train a HiddenMarkovModelTagger, using the train() method\n",
    "    tagger = HiddenMarkovModelTagger.train(train_data)\n",
    "\n",
    "    # TODO: using the hmm tagger tag the sentence\n",
    "    hmm_tagged_sentence = tagger.tag(sentence)\n",
    "\n",
    "    # TODO: using the hmm tagger, evaluate accuracy score on the test data\n",
    "    acc = tagger.evaluate(test_data)\n",
    "\n",
    "    return tagger, hmm_tagged_sentence, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_my_trainer():\n",
    "    tagged_sentences = brown.tagged_sents(categories='news')\n",
    "    words = [tp[0] for tp in tagged_sentences[42]]\n",
    "    tagger, hmm_tagged_sentence, acc = my_trainer(words, 500)\n",
    "    print('Training nltk.HiddenMarkovModelTagge with 500 sentences...')\n",
    "    print('\\tSentence tagged with model:')\n",
    "    pp.pprint(hmm_tagged_sentence)\n",
    "    print('\\tAccuracy score on the test set: %.4f%%' % (100.0*acc))\n",
    "    print()\n",
    "\n",
    "    tagger, hmm_tagged_sentence, acc = my_trainer(words, 3000)\n",
    "    print('Training nltk.HiddenMarkovModelTagge with 3000 sentences...')\n",
    "    print('\\tSentence tagged with model:')\n",
    "    pp.pprint(hmm_tagged_sentence)\n",
    "    print('\\tAccuracy score on the test set: %.4f%%' % (100.0*acc))\n",
    "\n",
    "test_my_trainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the tagged sentence and the accuracy of the tagger. How does the size of the training set affect the accuracy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Transition and Emission Probabilities\n",
    "\n",
    "In the previous exercise we learned how to train and evaluate an HMM tagger.\n",
    "We have used the HMM tagger as a black box and have seen how the training\n",
    "data affects the accuracy of the tagger. In order to get a better understanding\n",
    "of the HMM we will look at the two components of this model:\n",
    "    \n",
    "* The transition model\n",
    "* The emission model\n",
    "\n",
    "The transition model estimates $P (tag_{i+1} |tag_i )$, the probability of a POS tag\n",
    "at position $i+1$ given the previous tag (at position $i$). The emission model\n",
    "estimates $P (word|tag)$, the probability of the observed word given a tag.\n",
    "\n",
    "Given the above definitions, we will need to learn a Conditional Probability\n",
    "Distribution for each of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(nltk.probability.ConditionalProbDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Emission Model\n",
    "\n",
    "In this exercise we will estimate the emission model. In order to compute the\n",
    "Conditional Probability Distribution of $P (word|tag)$ we first have to compute\n",
    "the Conditional Frequency Distribution of a word given a tag.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(nltk.probability.ConditionalFreqDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructor of the ConditionalFreqDist class takes as input a list of tuples,\n",
    "each tuple consisting of a condition and an observation. For the emission model,\n",
    "the conditions are tags and the observations are the words. The template of the\n",
    "function that you have to implement takes as argument the list of tagged words\n",
    "from the Brown corpus.\n",
    "\n",
    "1. Build the dataset to be passed to the `ConditionalFreqDist()` constructor. Words should be lowercased. Each item of data should be a tuple of tag (a condition) and word (an observation).\n",
    "2. Compute the Conditional Frequency Distribution of words given tags.\n",
    "3. Return the top 10 most frequent words given the tag NN.\n",
    "4. Compute the Conditional Probability Distribution for the above Conditional Frequency Distribution. Use the `MLEProbDist` estimator when calling the ConditionalProbDist constructor.\n",
    "5. Compute the probabilities:\n",
    "\n",
    " $P(\\text{year}|\\text{NN})$ \n",
    " \n",
    " $P(\\text{year}|\\text{DT})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_emission_model(tagged_words):\n",
    "    \"\"\"Build and sample Conditional{Freq->Prob}Dist for word given tag from a list of tagged words using MLE\n",
    "    \n",
    "    :param tagged_words: tagged words (word,tag)\n",
    "    :type tagged_words: list(tuple(str,str))\n",
    "    :return: Conditional Freq dist of word given tag, top 10 words with tag NN,\n",
    "             Conditional Prob dist of word given tag, P('year'|'NN'), P('year'|'DT')\n",
    "    :rtype: tuple(nltk.probability.ConditionalFreqDist,list(tuple(str,int)),nltk.probability.ConditionalProbDist,float,float)\"\"\"\n",
    "    \n",
    "    # in the previous labs we've seen how to build a freq dist\n",
    "    # we need conditional distributions to estimate the transition and emission models\n",
    "    # in this exercise we estimate the emission model\n",
    "    \n",
    "    # TODO: prepare the data\n",
    "    # the data object should be a list of tuples of conditions and observations\n",
    "    # in our case the tuples should be of the form (tag,word) where words are lowercased\n",
    "    data = [ (tag, word.lower()) for (word, tag) in tagged_words]\n",
    "\n",
    "    # TODO: compute a Conditional Frequency Distribution for words given their tags using our data\n",
    "    emission_FD =ConditionalFreqDist(data)\n",
    "\n",
    "    # TODO: find the top 10 most frequent words given the tag NN\n",
    "    top_NN = emission_FD['NN'].most_common(10)\n",
    "\n",
    "    # TODO: Compute the Conditional Probability Distribution using the above Conditional Frequency Distribution. \n",
    "    #       Use nltk.probability.MLEProbDist estimator.\n",
    "    emission_PD =ConditionalProbDist(emission_FD,MLEProbDist)\n",
    "\n",
    "    # TODO: compute the probabilities of P(year|NN) and P(year|DT)\n",
    "    p_NN = emission_PD['NN'].prob('year')\n",
    "    p_DT = emission_PD['DT'].prob('year')\n",
    "\n",
    "    return emission_FD, top_NN, emission_PD, p_NN, p_DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_emission_model():\n",
    "    tagged_words = brown.tagged_words(categories='news')\n",
    "    (emission_FD, top_NN, emission_PD, p_NN, p_DT) = my_emission_model(tagged_words)\n",
    "    print('Frequency of words given the tag *NN*: ')\n",
    "    pp.pprint(top_NN)\n",
    "    print('P(year|NN) = ', p_NN)\n",
    "    print('P(year|DT) = ', p_DT)\n",
    "\n",
    "test_emission_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the estimated probabilities. Why is P(year|DT) = 0 ? \n",
    "\n",
    "What are the problems with having zero (0) probabilities and what can be done to\n",
    "avoid this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Transition Model\n",
    "\n",
    "In this exercise we will estimate the transition model. In order to compute the\n",
    "Conditional Probability Distribution of $P (tag_{i+1} |tag_i )$ we first have to compute\n",
    "the Conditional Frequency Distribution of a tag at position $i + 1$ given the previous tag.\n",
    "\n",
    "The constructor of the `ConditionalFreqDist` class takes as input a list of tuples, each tuple consisting of a condition and an observation. For the transition\n",
    "model, the conditions are tags at position i and the observations are tags at\n",
    "position $i + 1$. The template of the function that you have to implement takes\n",
    "as argument the list of tagged sentences from the Brown corpus.\n",
    "\n",
    "1. Build the dataset to be passed to the `ConditionalFreqDist()` constructor. Each item in your data should be a pair of condition and observation: $(tag_i,tag_{i+1})$\n",
    "2. Compute the Conditional Frequency Distribution of a tag at position $i + 1$ given the previous tag.\n",
    "3. Compute the Conditional Probability Distribution for the above Conditional Frequency Distribution. Use the `MLEProbDist` estimator when calling the `ConditionalProbDist` constructor.\n",
    "4. Compute the probabilities \n",
    "   \n",
    "   $P(\\text{NN}|\\text{VBD})$ \n",
    "   \n",
    "   $P(\\text{NN}|\\text{DT})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_transition_model(tagged_sentences):\n",
    "    \"\"\"Build and sample Conditional{Freq->Prob}Dist for tag given preceding tag from a list of tagged words using MLE\n",
    "    \n",
    "    :param tagged_sentences: Tagged sentences for training and testing\n",
    "    :type tagged_sentences: list(list(tuple(str,str)))\n",
    "    :return: Conditional Freq dist of tag given preceding tag,\n",
    "             Conditional Prob dist of tag given preceding tag, P('NN'|'VBD') and P('NN'|'DT')\n",
    "    :rtype: tuple(nltk.probability.ConditionalFreqDist,nltk.probability.ConditionalProbDist,float,float)\"\"\"\n",
    "    \n",
    "    # TODO: prepare the data\n",
    "    # the data object should be an array of tuples of conditions and observations\n",
    "    # in our case the tuples will be of the form (tag_(i),tag_(i+1))\n",
    "    tagGenerators=(((s[i][1],s[i+1][1]) for i in range(len(s)-1)) for s in tagged_sentences)\n",
    "    # tagGenerators is an iterator of iterators of pairs of tags\n",
    "    # The following chains them all together to produce an iterator of pairs of tags\n",
    "    data = itertools.chain.from_iterable(tagGenerators)\n",
    "\n",
    "    # TODO: compute a Conditional Frequency Distribution for a tag given the previous tag\n",
    "    transition_FD =ConditionalFreqDist(data)\n",
    "\n",
    "    # TODO: compute a Conditional Probability Distribution for the\n",
    "    # transition probability P(tag_(i+1)|tag_(i)) using an MLEProbDist\n",
    "    # to estimate the probabilities\n",
    "    transition_PD =ConditionalProbDist(transition_FD, MLEProbDist)\n",
    "\n",
    "    # TODO: compute the probabilities of P('NN'|'VBD') and P('NN'|'DT')\n",
    "    p_VBD_NN = transition_PD['VBD'].prob('NN')\n",
    "    p_DT_NN = transition_PD['DT'].prob('NN')\n",
    "\n",
    "    return transition_FD, transition_PD, p_VBD_NN, p_DT_NN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transition_model():\n",
    "    tagged_sentences = brown.tagged_sents(categories='news')\n",
    "    (transition_FD, transition_PD, p_VBD_NN, p_DT_NN) = my_transition_model(tagged_sentences)\n",
    "    print('P(NN|VBD) = ', p_VBD_NN)\n",
    "    print('P(NN|DT) = ', p_DT_NN)\n",
    "    \n",
    "test_transition_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the results what you would expect? The sequence NN DT seems very probable. \n",
    "\n",
    "How will this affect the sequence tagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going further\n",
    "\n",
    "Modify your code for exercise 3 to use a different estimator, to introduce some\n",
    "smoothing, and compare the results with the original.\n",
    "In exercise 4 we didnâ€™t do anything about the boundaries. Modify your code for\n",
    "exercise 4 to use `<s>` at the beginning of every sentence and `</s>` at the end.\n",
    "\n",
    "Explore the resulting conditional probabilities. What is the most likely tag at\n",
    "the beginning of a sentence? At the end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
