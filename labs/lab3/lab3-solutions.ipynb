{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNLP: Lab Session 3\n",
    "\n",
    "# Hidden Markov Models - Construction and Use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages used for this lab\n",
    "\n",
    "import nltk\n",
    "\n",
    "# import brown corpus\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# module for training a Hidden Markov Model and tagging sequences\n",
    "from nltk.tag.hmm import HiddenMarkovModelTagger\n",
    "\n",
    "# module for computing a Conditional Frequency Distribution\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "# module for computing a Conditional Probability Distribution\n",
    "from nltk.probability import ConditionalProbDist\n",
    "\n",
    "# module for computing a probability distribution with the Maximum Likelihood Estimate\n",
    "from nltk.probability import MLEProbDist\n",
    "\n",
    "# pretty printing\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Corpora tagged with Part-of-Speech information\n",
    "\n",
    "NLTK provides corpora annotated with Part-of-Speech (POS) information and\n",
    "some tools to access this information. The Penn Treebank tagset is commonly\n",
    "used for annotating English sentences. We can inspect this tagset in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown corpus provided with NLTK is also tagged with POS information,\n",
    "although the tagset is slightly different than the Penn Treebank tagset. Information about the Brown corpus tagset can be found here:\n",
    "http://www.scs.leeds.ac.uk/ccalas/tagsets/brown.html\n",
    "\n",
    "We can retrieve the tagged sentences in the Brown corpus by calling the `tagged_sents()`\n",
    "function and looking at an annotated sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tagged with Penn Treebank POS labels:\n",
      "[   ('The', 'AT'),\n",
      "    ('jury', 'NN'),\n",
      "    ('praised', 'VBD'),\n",
      "    ('the', 'AT'),\n",
      "    ('administration', 'NN'),\n",
      "    ('and', 'CC'),\n",
      "    ('operation', 'NN'),\n",
      "    ('of', 'IN'),\n",
      "    ('the', 'AT'),\n",
      "    ('Atlanta', 'NP-TL'),\n",
      "    ('Police', 'NNS-TL'),\n",
      "    ('Department', 'NN-TL'),\n",
      "    (',', ','),\n",
      "    ('the', 'AT'),\n",
      "    ('Fulton', 'NP-TL'),\n",
      "    ('Tax', 'NN-TL'),\n",
      "    (\"Commissioner's\", 'NN$-TL'),\n",
      "    ('Office', 'NN-TL'),\n",
      "    (',', ','),\n",
      "    ('the', 'AT'),\n",
      "    ('Bellwood', 'NP'),\n",
      "    ('and', 'CC'),\n",
      "    ('Alpharetta', 'NP'),\n",
      "    ('prison', 'NN'),\n",
      "    ('farms', 'NNS'),\n",
      "    (',', ','),\n",
      "    ('Grady', 'NP-TL'),\n",
      "    ('Hospital', 'NN-TL'),\n",
      "    ('and', 'CC'),\n",
      "    ('the', 'AT'),\n",
      "    ('Fulton', 'NP-TL'),\n",
      "    ('Health', 'NN-TL'),\n",
      "    ('Department', 'NN-TL'),\n",
      "    ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences = brown.tagged_sents(categories='news')\n",
    "print('Sentence tagged with Penn Treebank POS labels:')\n",
    "pp.pprint(tagged_sentences[29])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is useful to use a coarser label set in order to avoid data sparsity\n",
    "or to allow a mapping between the POS labels for different languages. The Universal tagset was designed to be applicable for all languages:\n",
    "\n",
    "https://github.com/slavpetrov/universal-pos-tags\n",
    "\n",
    "There are mappings between the POS tagset of several languages and the Universal tagset. We can access the Universal tags for the Brown corpus sentences\n",
    "by changing the tagset argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tagged with Universal POS:\n",
      "[   ('The', 'DET'),\n",
      "    ('jury', 'NOUN'),\n",
      "    ('praised', 'VERB'),\n",
      "    ('the', 'DET'),\n",
      "    ('administration', 'NOUN'),\n",
      "    ('and', 'CONJ'),\n",
      "    ('operation', 'NOUN'),\n",
      "    ('of', 'ADP'),\n",
      "    ('the', 'DET'),\n",
      "    ('Atlanta', 'NOUN'),\n",
      "    ('Police', 'NOUN'),\n",
      "    ('Department', 'NOUN'),\n",
      "    (',', '.'),\n",
      "    ('the', 'DET'),\n",
      "    ('Fulton', 'NOUN'),\n",
      "    ('Tax', 'NOUN'),\n",
      "    (\"Commissioner's\", 'NOUN'),\n",
      "    ('Office', 'NOUN'),\n",
      "    (',', '.'),\n",
      "    ('the', 'DET'),\n",
      "    ('Bellwood', 'NOUN'),\n",
      "    ('and', 'CONJ'),\n",
      "    ('Alpharetta', 'NOUN'),\n",
      "    ('prison', 'NOUN'),\n",
      "    ('farms', 'NOUN'),\n",
      "    (',', '.'),\n",
      "    ('Grady', 'NOUN'),\n",
      "    ('Hospital', 'NOUN'),\n",
      "    ('and', 'CONJ'),\n",
      "    ('the', 'DET'),\n",
      "    ('Fulton', 'NOUN'),\n",
      "    ('Health', 'NOUN'),\n",
      "    ('Department', 'NOUN'),\n",
      "    ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences_universal = brown.tagged_sents(categories='news', tagset='universal')\n",
    "print('Sentence tagged with Universal POS:')\n",
    "pp.pprint(tagged_sentences_universal[29])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initial universal tagset was later expanded as part of the Universal Dependencies project. The resulting tagset is called UPOS and you can find more information in the link below. This tagset is not yet supported by NLTK. However, it is important that you know about it since it is the most used multi-lingual tagset nowadays.\n",
    "\n",
    "https://universaldependencies.org/u/pos/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Exploring Brown corpus\n",
    "\n",
    "In this exercise we will explore the Brown corpus, specifically its frequency distribution over POS tags.\n",
    "The Brown corpus is divided in topical categories called 'genres'. Let's see what genres we have in the corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   'adventure',\n",
      "    'belles_lettres',\n",
      "    'editorial',\n",
      "    'fiction',\n",
      "    'government',\n",
      "    'hobbies',\n",
      "    'humor',\n",
      "    'learned',\n",
      "    'lore',\n",
      "    'mystery',\n",
      "    'news',\n",
      "    'religion',\n",
      "    'reviews',\n",
      "    'romance',\n",
      "    'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(brown.categories())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You task in this exercise is to implement a function that computes the Frequency Distribution over a Brown genre category and a tagset scheme.\n",
    "The template of the function is given below. It takes two parameters: one is the genre category and the other is the tagset name.\n",
    "Your job is to do the following:\n",
    "\n",
    "1. Convert the list of (word,tag) pairs to a list of tags\n",
    "2. Use the list of tags to compute a frequency distribution over the tags. Use NLTK's `FreqDist()`\n",
    "3. Compute the total number of tags in the Frequency Distribution\n",
    "4. Return the total number of tags and the top 10 most frequent tags\n",
    "\n",
    "You are given the code to retrieve the list of (word, tag) tuples from the brown corpus corresponding to the given category and tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_tagset_frequency_distro(genre, tagset):\n",
    "    \"\"\"Compute a Frequency distribution of the POS tags in a genre of the tagged Brown corpus\n",
    "    \n",
    "    :param genre: A Brown corpus genre\n",
    "    :type genre: str or iterable(str) or None\n",
    "    :param tagset: A Brown tagset name\n",
    "    :type tagset: str or None (defaults to 'brown')\n",
    "    :return: number of tag types, top 10 tags\n",
    "    :rtype: tuple(int,list(tuple(str,int))\"\"\"\n",
    "\n",
    "    # get the tagged words from the corpus\n",
    "    tagged_words = brown.tagged_words(categories=genre, tagset=tagset)\n",
    "\n",
    "    # TODO: convert tagged_words to a list of tags\n",
    "    tags = ( tp[1] for tp in tagged_words)\n",
    "\n",
    "    # TODO: using the above list compute a Frequency Distribution\n",
    "    # hint: use nltk.FreqDist()\n",
    "    tagsFDist = nltk.FreqDist(tags)\n",
    "\n",
    "    # TODO: retrieve the number of tag types in the tagset\n",
    "    # hint: help(nltk.FreqDist)\n",
    "    number_of_tags = tagsFDist.B()\n",
    "\n",
    "    # TODO: retrieve the top 10 most frequent tags and their counts\n",
    "    top_tags = tagsFDist.most_common(10)\n",
    "\n",
    "    return number_of_tags, top_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class FreqDist in module nltk.probability:\n",
      "\n",
      "class FreqDist(collections.Counter)\n",
      " |  A frequency distribution for the outcomes of an experiment.  A\n",
      " |  frequency distribution records the number of times each outcome of\n",
      " |  an experiment has occurred.  For example, a frequency distribution\n",
      " |  could be used to record the frequency of each word type in a\n",
      " |  document.  Formally, a frequency distribution can be defined as a\n",
      " |  function mapping from each sample to the number of times that\n",
      " |  sample occurred as an outcome.\n",
      " |  \n",
      " |  Frequency distributions are generally constructed by running a\n",
      " |  number of experiments, and incrementing the count for a sample\n",
      " |  every time it is an outcome of an experiment.  For example, the\n",
      " |  following code will produce a frequency distribution that encodes\n",
      " |  how often each word occurs in a text:\n",
      " |  \n",
      " |      >>> from nltk.tokenize import word_tokenize\n",
      " |      >>> from nltk.probability import FreqDist\n",
      " |      >>> sent = 'This is an example sentence'\n",
      " |      >>> fdist = FreqDist()\n",
      " |      >>> for word in word_tokenize(sent):\n",
      " |      ...    fdist[word.lower()] += 1\n",
      " |  \n",
      " |  An equivalent way to do this is with the initializer:\n",
      " |  \n",
      " |      >>> fdist = FreqDist(word.lower() for word in word_tokenize(sent))\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      FreqDist\n",
      " |      collections.Counter\n",
      " |      builtins.dict\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  B(self)\n",
      " |      Return the total number of sample values (or \"bins\") that\n",
      " |      have counts greater than zero.  For the total\n",
      " |      number of sample outcomes recorded, use ``FreqDist.N()``.\n",
      " |      (FreqDist.B() is the same as len(FreqDist).)\n",
      " |      \n",
      " |      :rtype: int\n",
      " |  \n",
      " |  N(self)\n",
      " |      Return the total number of sample outcomes that have been\n",
      " |      recorded by this FreqDist.  For the number of unique\n",
      " |      sample values (or bins) with counts greater than zero, use\n",
      " |      ``FreqDist.B()``.\n",
      " |      \n",
      " |      :rtype: int\n",
      " |  \n",
      " |  Nr(self, r, bins=None)\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      Add counts from two counters.\n",
      " |      \n",
      " |      >>> FreqDist('abbb') + FreqDist('bcc')\n",
      " |      FreqDist({'b': 4, 'c': 2, 'a': 1})\n",
      " |  \n",
      " |  __and__(self, other)\n",
      " |      Intersection is the minimum of corresponding counts.\n",
      " |      \n",
      " |      >>> FreqDist('abbb') & FreqDist('bcc')\n",
      " |      FreqDist({'b': 1})\n",
      " |  \n",
      " |  __delitem__(self, key)\n",
      " |      Override ``Counter.__delitem__()`` to invalidate the cached N\n",
      " |  \n",
      " |  __ge__ lambda self, other\n",
      " |      # @total_ordering doesn't work here, since the class inherits from a builtin class\n",
      " |  \n",
      " |  __gt__ lambda self, other\n",
      " |  \n",
      " |  __init__(self, samples=None)\n",
      " |      Construct a new frequency distribution.  If ``samples`` is\n",
      " |      given, then the frequency distribution will be initialized\n",
      " |      with the count of each object in ``samples``; otherwise, it\n",
      " |      will be initialized to be empty.\n",
      " |      \n",
      " |      In particular, ``FreqDist()`` returns an empty frequency\n",
      " |      distribution; and ``FreqDist(samples)`` first creates an empty\n",
      " |      frequency distribution, and then calls ``update`` with the\n",
      " |      list ``samples``.\n",
      " |      \n",
      " |      :param samples: The samples to initialize the frequency\n",
      " |          distribution with.\n",
      " |      :type samples: Sequence\n",
      " |  \n",
      " |  __le__(self, other)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __lt__ lambda self, other\n",
      " |  \n",
      " |  __or__(self, other)\n",
      " |      Union is the maximum of value in either of the input counters.\n",
      " |      \n",
      " |      >>> FreqDist('abbb') | FreqDist('bcc')\n",
      " |      FreqDist({'b': 3, 'c': 2, 'a': 1})\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return a string representation of this FreqDist.\n",
      " |      \n",
      " |      :rtype: string\n",
      " |  \n",
      " |  __setitem__(self, key, val)\n",
      " |      Override ``Counter.__setitem__()`` to invalidate the cached N\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return a string representation of this FreqDist.\n",
      " |      \n",
      " |      :rtype: string\n",
      " |  \n",
      " |  __sub__(self, other)\n",
      " |      Subtract count, but keep only results with positive counts.\n",
      " |      \n",
      " |      >>> FreqDist('abbbc') - FreqDist('bccd')\n",
      " |      FreqDist({'b': 2, 'a': 1})\n",
      " |  \n",
      " |  __unicode__ = __str__(self)\n",
      " |  \n",
      " |  copy(self)\n",
      " |      Create a copy of this frequency distribution.\n",
      " |      \n",
      " |      :rtype: FreqDist\n",
      " |  \n",
      " |  freq(self, sample)\n",
      " |      Return the frequency of a given sample.  The frequency of a\n",
      " |      sample is defined as the count of that sample divided by the\n",
      " |      total number of sample outcomes that have been recorded by\n",
      " |      this FreqDist.  The count of a sample is defined as the\n",
      " |      number of times that sample outcome was recorded by this\n",
      " |      FreqDist.  Frequencies are always real numbers in the range\n",
      " |      [0, 1].\n",
      " |      \n",
      " |      :param sample: the sample whose frequency\n",
      " |             should be returned.\n",
      " |      :type sample: any\n",
      " |      :rtype: float\n",
      " |  \n",
      " |  hapaxes(self)\n",
      " |      Return a list of all samples that occur once (hapax legomena)\n",
      " |      \n",
      " |      :rtype: list\n",
      " |  \n",
      " |  max(self)\n",
      " |      Return the sample with the greatest number of outcomes in this\n",
      " |      frequency distribution.  If two or more samples have the same\n",
      " |      number of outcomes, return one of them; which sample is\n",
      " |      returned is undefined.  If no outcomes have occurred in this\n",
      " |      frequency distribution, return None.\n",
      " |      \n",
      " |      :return: The sample with the maximum number of outcomes in this\n",
      " |              frequency distribution.\n",
      " |      :rtype: any or None\n",
      " |  \n",
      " |  pformat(self, maxlen=10)\n",
      " |      Return a string representation of this FreqDist.\n",
      " |      \n",
      " |      :param maxlen: The maximum number of items to display\n",
      " |      :type maxlen: int\n",
      " |      :rtype: string\n",
      " |  \n",
      " |  plot(self, *args, **kwargs)\n",
      " |      Plot samples from the frequency distribution\n",
      " |      displaying the most frequent sample first.  If an integer\n",
      " |      parameter is supplied, stop after this many samples have been\n",
      " |      plotted.  For a cumulative plot, specify cumulative=True.\n",
      " |      (Requires Matplotlib to be installed.)\n",
      " |      \n",
      " |      :param title: The title for the graph\n",
      " |      :type title: str\n",
      " |      :param cumulative: A flag to specify whether the plot is cumulative (default = False)\n",
      " |      :type title: bool\n",
      " |  \n",
      " |  pprint(self, maxlen=10, stream=None)\n",
      " |      Print a string representation of this FreqDist to 'stream'\n",
      " |      \n",
      " |      :param maxlen: The maximum number of items to print\n",
      " |      :type maxlen: int\n",
      " |      :param stream: The stream to print to. stdout by default\n",
      " |  \n",
      " |  r_Nr(self, bins=None)\n",
      " |      Return the dictionary mapping r to Nr, the number of samples with frequency r, where Nr > 0.\n",
      " |      \n",
      " |      :type bins: int\n",
      " |      :param bins: The number of possible sample outcomes.  ``bins``\n",
      " |          is used to calculate Nr(0).  In particular, Nr(0) is\n",
      " |          ``bins-self.B()``.  If ``bins`` is not specified, it\n",
      " |          defaults to ``self.B()`` (so Nr(0) will be 0).\n",
      " |      :rtype: int\n",
      " |  \n",
      " |  setdefault(self, key, val)\n",
      " |      Override ``Counter.setdefault()`` to invalidate the cached N\n",
      " |  \n",
      " |  tabulate(self, *args, **kwargs)\n",
      " |      Tabulate the given samples from the frequency distribution (cumulative),\n",
      " |      displaying the most frequent sample first.  If an integer\n",
      " |      parameter is supplied, stop after this many samples have been\n",
      " |      plotted.\n",
      " |      \n",
      " |      :param samples: The samples to plot (default is all samples)\n",
      " |      :type samples: list\n",
      " |      :param cumulative: A flag to specify whether the freqs are cumulative (default = False)\n",
      " |      :type title: bool\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |  \n",
      " |  update(self, *args, **kwargs)\n",
      " |      Override ``Counter.update()`` to invalidate the cached N\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from collections.Counter:\n",
      " |  \n",
      " |  __iadd__(self, other)\n",
      " |      Inplace add from another counter, keeping only positive counts.\n",
      " |      \n",
      " |      >>> c = Counter('abbb')\n",
      " |      >>> c += Counter('bcc')\n",
      " |      >>> c\n",
      " |      Counter({'b': 4, 'c': 2, 'a': 1})\n",
      " |  \n",
      " |  __iand__(self, other)\n",
      " |      Inplace intersection is the minimum of corresponding counts.\n",
      " |      \n",
      " |      >>> c = Counter('abbb')\n",
      " |      >>> c &= Counter('bcc')\n",
      " |      >>> c\n",
      " |      Counter({'b': 1})\n",
      " |  \n",
      " |  __ior__(self, other)\n",
      " |      Inplace union is the maximum of value from either counter.\n",
      " |      \n",
      " |      >>> c = Counter('abbb')\n",
      " |      >>> c |= Counter('bcc')\n",
      " |      >>> c\n",
      " |      Counter({'b': 3, 'c': 2, 'a': 1})\n",
      " |  \n",
      " |  __isub__(self, other)\n",
      " |      Inplace subtract counter, but keep only results with positive counts.\n",
      " |      \n",
      " |      >>> c = Counter('abbbc')\n",
      " |      >>> c -= Counter('bccd')\n",
      " |      >>> c\n",
      " |      Counter({'b': 2, 'a': 1})\n",
      " |  \n",
      " |  __missing__(self, key)\n",
      " |      The count of elements not in the Counter is zero.\n",
      " |  \n",
      " |  __neg__(self)\n",
      " |      Subtracts from an empty counter.  Strips positive and zero counts,\n",
      " |      and flips the sign on negative counts.\n",
      " |  \n",
      " |  __pos__(self)\n",
      " |      Adds an empty counter, effectively stripping negative and zero counts\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      helper for pickle\n",
      " |  \n",
      " |  elements(self)\n",
      " |      Iterator over elements repeating each as many times as its count.\n",
      " |      \n",
      " |      >>> c = Counter('ABCABC')\n",
      " |      >>> sorted(c.elements())\n",
      " |      ['A', 'A', 'B', 'B', 'C', 'C']\n",
      " |      \n",
      " |      # Knuth's example for prime factors of 1836:  2**2 * 3**3 * 17**1\n",
      " |      >>> prime_factors = Counter({2: 2, 3: 3, 17: 1})\n",
      " |      >>> product = 1\n",
      " |      >>> for factor in prime_factors.elements():     # loop over factors\n",
      " |      ...     product *= factor                       # and multiply them\n",
      " |      >>> product\n",
      " |      1836\n",
      " |      \n",
      " |      Note, if an element's count has been set to zero or is a negative\n",
      " |      number, elements() will ignore it.\n",
      " |  \n",
      " |  most_common(self, n=None)\n",
      " |      List the n most common elements and their counts from the most\n",
      " |      common to the least.  If n is None, then list all element counts.\n",
      " |      \n",
      " |      >>> Counter('abcdeabcdabcaba').most_common(3)\n",
      " |      [('a', 5), ('b', 4), ('c', 3)]\n",
      " |  \n",
      " |  subtract(*args, **kwds)\n",
      " |      Like dict.update() but subtracts counts instead of replacing them.\n",
      " |      Counts can be reduced below zero.  Both the inputs and outputs are\n",
      " |      allowed to contain zero and negative counts.\n",
      " |      \n",
      " |      Source can be an iterable, a dictionary, or another Counter instance.\n",
      " |      \n",
      " |      >>> c = Counter('which')\n",
      " |      >>> c.subtract('witch')             # subtract elements from another iterable\n",
      " |      >>> c.subtract(Counter('watch'))    # subtract elements from another counter\n",
      " |      >>> c['h']                          # 2 in which, minus 1 in witch, minus 1 in watch\n",
      " |      0\n",
      " |      >>> c['w']                          # 1 in which, minus 1 in witch, minus 1 in watch\n",
      " |      -1\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from collections.Counter:\n",
      " |  \n",
      " |  fromkeys(iterable, v=None) from builtins.type\n",
      " |      Returns a new dict with keys from iterable and values equal to value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from collections.Counter:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.dict:\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      True if D has a key k, else False.\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      D.__sizeof__() -> size of D in memory, in bytes\n",
      " |  \n",
      " |  clear(...)\n",
      " |      D.clear() -> None.  Remove all items from D.\n",
      " |  \n",
      " |  get(...)\n",
      " |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
      " |  \n",
      " |  items(...)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  keys(...)\n",
      " |      D.keys() -> a set-like object providing a view on D's keys\n",
      " |  \n",
      " |  pop(...)\n",
      " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      " |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      " |  \n",
      " |  popitem(...)\n",
      " |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
      " |      2-tuple; but raise KeyError if D is empty.\n",
      " |  \n",
      " |  values(...)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from builtins.dict:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.FreqDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code with this function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag FreqDist for news with Penn Treebank tagset:\n",
      "(   218,\n",
      "    [   ('NN', 13162),\n",
      "        ('IN', 10616),\n",
      "        ('AT', 8893),\n",
      "        ('NP', 6866),\n",
      "        (',', 5133),\n",
      "        ('NNS', 5066),\n",
      "        ('.', 4452),\n",
      "        ('JJ', 4392),\n",
      "        ('CC', 2664),\n",
      "        ('VBD', 2524)])\n",
      "Tag FreqDist for science_fiction with Penn Treebank tagset:\n",
      "(   127,\n",
      "    [   ('NN', 1541),\n",
      "        ('IN', 1176),\n",
      "        ('.', 1077),\n",
      "        ('AT', 1040),\n",
      "        (',', 791),\n",
      "        ('JJ', 723),\n",
      "        ('NNS', 532),\n",
      "        ('VBD', 531),\n",
      "        ('RB', 522),\n",
      "        ('VB', 495)])\n",
      "Tag FreqDist for news with Universal tagset:\n",
      "(   12,\n",
      "    [   ('NOUN', 30640),\n",
      "        ('VERB', 14399),\n",
      "        ('ADP', 12355),\n",
      "        ('.', 11928),\n",
      "        ('DET', 11389),\n",
      "        ('ADJ', 6706),\n",
      "        ('ADV', 3349),\n",
      "        ('CONJ', 2717),\n",
      "        ('PRON', 2535),\n",
      "        ('PRT', 2264)])\n",
      "Tag FreqDist for science_fiction with Universal tagset:\n",
      "(   12,\n",
      "    [   ('NOUN', 2747),\n",
      "        ('VERB', 2579),\n",
      "        ('.', 2428),\n",
      "        ('DET', 1582),\n",
      "        ('ADP', 1451),\n",
      "        ('PRON', 934),\n",
      "        ('ADJ', 929),\n",
      "        ('ADV', 828),\n",
      "        ('PRT', 483),\n",
      "        ('CONJ', 416)])\n"
     ]
    }
   ],
   "source": [
    "def test_ex1():\n",
    "    print('Tag FreqDist for news with Penn Treebank tagset:')\n",
    "    pp.pprint(explore_tagset_frequency_distro('news', None))\n",
    "\n",
    "    print('Tag FreqDist for science_fiction with Penn Treebank tagset:')\n",
    "    pp.pprint(explore_tagset_frequency_distro('science_fiction', None))\n",
    "\n",
    "    # Do the same thing for a different tagset: Universal\n",
    "\n",
    "    print('Tag FreqDist for news with Universal tagset:')\n",
    "    pp.pprint(explore_tagset_frequency_distro('news', 'universal'))\n",
    "\n",
    "    print('Tag FreqDist for science_fiction with Universal tagset:')\n",
    "    pp.pprint(explore_tagset_frequency_distro('science_fiction', 'universal'))\n",
    "\n",
    "test_ex1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the top tags for different genre and tagsets. Observe differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating an HMM Tagger\n",
    "\n",
    "NLTK provides a module for training a Hidden Markov Model for sequence tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class HiddenMarkovModelTagger in module nltk.tag.hmm:\n",
      "\n",
      "class HiddenMarkovModelTagger(nltk.tag.api.TaggerI)\n",
      " |  Hidden Markov model class, a generative model for labelling sequence data.\n",
      " |  These models define the joint probability of a sequence of symbols and\n",
      " |  their labels (state transitions) as the product of the starting state\n",
      " |  probability, the probability of each state transition, and the probability\n",
      " |  of each observation being generated from each state. This is described in\n",
      " |  more detail in the module documentation.\n",
      " |  \n",
      " |  This implementation is based on the HMM description in Chapter 8, Huang,\n",
      " |  Acero and Hon, Spoken Language Processing and includes an extension for\n",
      " |  training shallow HMM parsers or specialized HMMs as in Molina et.\n",
      " |  al, 2002.  A specialized HMM modifies training data by applying a\n",
      " |  specialization function to create a new training set that is more\n",
      " |  appropriate for sequential tagging with an HMM.  A typical use case is\n",
      " |  chunking.\n",
      " |  \n",
      " |  :param symbols: the set of output symbols (alphabet)\n",
      " |  :type symbols: seq of any\n",
      " |  :param states: a set of states representing state space\n",
      " |  :type states: seq of any\n",
      " |  :param transitions: transition probabilities; Pr(s_i | s_j) is the\n",
      " |      probability of transition from state i given the model is in\n",
      " |      state_j\n",
      " |  :type transitions: ConditionalProbDistI\n",
      " |  :param outputs: output probabilities; Pr(o_k | s_i) is the probability\n",
      " |      of emitting symbol k when entering state i\n",
      " |  :type outputs: ConditionalProbDistI\n",
      " |  :param priors: initial state distribution; Pr(s_i) is the probability\n",
      " |      of starting in state i\n",
      " |  :type priors: ProbDistI\n",
      " |  :param transform: an optional function for transforming training\n",
      " |      instances, defaults to the identity function.\n",
      " |  :type transform: callable\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      HiddenMarkovModelTagger\n",
      " |      nltk.tag.api.TaggerI\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, symbols, states, transitions, outputs, priors, transform=<function _identity at 0x7f8dc4583950>)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __unicode__ = __str__(self, /)\n",
      " |  \n",
      " |  best_path(self, unlabeled_sequence)\n",
      " |      Returns the state sequence of the optimal (most probable) path through\n",
      " |      the HMM. Uses the Viterbi algorithm to calculate this part by dynamic\n",
      " |      programming.\n",
      " |      \n",
      " |      :return: the state sequence\n",
      " |      :rtype: sequence of any\n",
      " |      :param unlabeled_sequence: the sequence of unlabeled symbols\n",
      " |      :type unlabeled_sequence: list\n",
      " |  \n",
      " |  best_path_simple(self, unlabeled_sequence)\n",
      " |      Returns the state sequence of the optimal (most probable) path through\n",
      " |      the HMM. Uses the Viterbi algorithm to calculate this part by dynamic\n",
      " |      programming.  This uses a simple, direct method, and is included for\n",
      " |      teaching purposes.\n",
      " |      \n",
      " |      :return: the state sequence\n",
      " |      :rtype: sequence of any\n",
      " |      :param unlabeled_sequence: the sequence of unlabeled symbols\n",
      " |      :type unlabeled_sequence: list\n",
      " |  \n",
      " |  entropy(self, unlabeled_sequence)\n",
      " |      Returns the entropy over labellings of the given sequence. This is\n",
      " |      given by::\n",
      " |      \n",
      " |          H(O) = - sum_S Pr(S | O) log Pr(S | O)\n",
      " |      \n",
      " |      where the summation ranges over all state sequences, S. Let\n",
      " |      *Z = Pr(O) = sum_S Pr(S, O)}* where the summation ranges over all state\n",
      " |      sequences and O is the observation sequence. As such the entropy can\n",
      " |      be re-expressed as::\n",
      " |      \n",
      " |          H = - sum_S Pr(S | O) log [ Pr(S, O) / Z ]\n",
      " |          = log Z - sum_S Pr(S | O) log Pr(S, 0)\n",
      " |          = log Z - sum_S Pr(S | O) [ log Pr(S_0) + sum_t Pr(S_t | S_{t-1}) + sum_t Pr(O_t | S_t) ]\n",
      " |      \n",
      " |      The order of summation for the log terms can be flipped, allowing\n",
      " |      dynamic programming to be used to calculate the entropy. Specifically,\n",
      " |      we use the forward and backward probabilities (alpha, beta) giving::\n",
      " |      \n",
      " |          H = log Z - sum_s0 alpha_0(s0) beta_0(s0) / Z * log Pr(s0)\n",
      " |          + sum_t,si,sj alpha_t(si) Pr(sj | si) Pr(O_t+1 | sj) beta_t(sj) / Z * log Pr(sj | si)\n",
      " |          + sum_t,st alpha_t(st) beta_t(st) / Z * log Pr(O_t | st)\n",
      " |      \n",
      " |      This simply uses alpha and beta to find the probabilities of partial\n",
      " |      sequences, constrained to include the given state(s) at some point in\n",
      " |      time.\n",
      " |  \n",
      " |  log_probability(self, sequence)\n",
      " |      Returns the log-probability of the given symbol sequence. If the\n",
      " |      sequence is labelled, then returns the joint log-probability of the\n",
      " |      symbol, state sequence. Otherwise, uses the forward algorithm to find\n",
      " |      the log-probability over all label sequences.\n",
      " |      \n",
      " |      :return: the log-probability of the sequence\n",
      " |      :rtype: float\n",
      " |      :param sequence: the sequence of symbols which must contain the TEXT\n",
      " |          property, and optionally the TAG property\n",
      " |      :type sequence:  Token\n",
      " |  \n",
      " |  point_entropy(self, unlabeled_sequence)\n",
      " |      Returns the pointwise entropy over the possible states at each\n",
      " |      position in the chain, given the observation sequence.\n",
      " |  \n",
      " |  probability(self, sequence)\n",
      " |      Returns the probability of the given symbol sequence. If the sequence\n",
      " |      is labelled, then returns the joint probability of the symbol, state\n",
      " |      sequence. Otherwise, uses the forward algorithm to find the\n",
      " |      probability over all label sequences.\n",
      " |      \n",
      " |      :return: the probability of the sequence\n",
      " |      :rtype: float\n",
      " |      :param sequence: the sequence of symbols which must contain the TEXT\n",
      " |          property, and optionally the TAG property\n",
      " |      :type sequence:  Token\n",
      " |  \n",
      " |  random_sample(self, rng, length)\n",
      " |      Randomly sample the HMM to generate a sentence of a given length. This\n",
      " |      samples the prior distribution then the observation distribution and\n",
      " |      transition distribution for each subsequent observation and state.\n",
      " |      This will mostly generate unintelligible garbage, but can provide some\n",
      " |      amusement.\n",
      " |      \n",
      " |      :return:        the randomly created state/observation sequence,\n",
      " |                      generated according to the HMM's probability\n",
      " |                      distributions. The SUBTOKENS have TEXT and TAG\n",
      " |                      properties containing the observation and state\n",
      " |                      respectively.\n",
      " |      :rtype:         list\n",
      " |      :param rng:     random number generator\n",
      " |      :type rng:      Random (or any object with a random() method)\n",
      " |      :param length:  desired output length\n",
      " |      :type length:   int\n",
      " |  \n",
      " |  reset_cache(self)\n",
      " |  \n",
      " |  tag(self, unlabeled_sequence)\n",
      " |      Tags the sequence with the highest probability state sequence. This\n",
      " |      uses the best_path method to find the Viterbi path.\n",
      " |      \n",
      " |      :return: a labelled sequence of symbols\n",
      " |      :rtype: list\n",
      " |      :param unlabeled_sequence: the sequence of unlabeled symbols\n",
      " |      :type unlabeled_sequence: list\n",
      " |  \n",
      " |  test(self, test_sequence, verbose=False, **kwargs)\n",
      " |      Tests the HiddenMarkovModelTagger instance.\n",
      " |      \n",
      " |      :param test_sequence: a sequence of labeled test instances\n",
      " |      :type test_sequence: list(list)\n",
      " |      :param verbose: boolean flag indicating whether training should be\n",
      " |          verbose or include printed output\n",
      " |      :type verbose: bool\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  train(labeled_sequence, test_sequence=None, unlabeled_sequence=None, **kwargs) from abc.ABCMeta\n",
      " |      Train a new HiddenMarkovModelTagger using the given labeled and\n",
      " |      unlabeled training instances. Testing will be performed if test\n",
      " |      instances are provided.\n",
      " |      \n",
      " |      :return: a hidden markov model tagger\n",
      " |      :rtype: HiddenMarkovModelTagger\n",
      " |      :param labeled_sequence: a sequence of labeled training instances,\n",
      " |          i.e. a list of sentences represented as tuples\n",
      " |      :type labeled_sequence: list(list)\n",
      " |      :param test_sequence: a sequence of labeled test instances\n",
      " |      :type test_sequence: list(list)\n",
      " |      :param unlabeled_sequence: a sequence of unlabeled training instances,\n",
      " |          i.e. a list of sentences represented as words\n",
      " |      :type unlabeled_sequence: list(list)\n",
      " |      :param transform: an optional function for transforming training\n",
      " |          instances, defaults to the identity function, see ``transform()``\n",
      " |      :type transform: function\n",
      " |      :param estimator: an optional function or class that maps a\n",
      " |          condition's frequency distribution to its probability\n",
      " |          distribution, defaults to a Lidstone distribution with gamma = 0.1\n",
      " |      :type estimator: class or function\n",
      " |      :param verbose: boolean flag indicating whether training should be\n",
      " |          verbose or include printed output\n",
      " |      :type verbose: bool\n",
      " |      :param max_iterations: number of Baum-Welch interations to perform\n",
      " |      :type max_iterations: int\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.tag.api.TaggerI:\n",
      " |  \n",
      " |  evaluate(self, gold)\n",
      " |      Score the accuracy of the tagger against the gold standard.\n",
      " |      Strip the tags from the gold standard text, retag it using\n",
      " |      the tagger, then compute the accuracy score.\n",
      " |      \n",
      " |      :type gold: list(list(tuple(str, str)))\n",
      " |      :param gold: The list of tagged sentences to score the tagger on.\n",
      " |      :rtype: float\n",
      " |  \n",
      " |  tag_sents(self, sentences)\n",
      " |      Apply ``self.tag()`` to each element of *sentences*.  I.e.:\n",
      " |      \n",
      " |          return [self.tag(sent) for sent in sentences]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nltk.tag.api.TaggerI:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.tag.hmm.HiddenMarkovModelTagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train the HMM for POS tagging given a labelled dataset. At the begging of this lab we learned how to access the labelled sentences of the Brown corpus.\n",
    "We will use this dataset to study the effect of the size of the training corpus on\n",
    "the accuracy of the tagger.\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "In this exercise we will train a HMM tagger on a training set and evaluate it\n",
    "on a test set. The template of the function that you have to implement takes\n",
    "two parameters: a sentence to be tagged and the size of the training corpus in\n",
    "number of sentences. You are given the code that creates the training and test\n",
    "datasets from the tagged sentences in the Brown corpus.\n",
    "\n",
    "1. Train a Hidden Markov Model tagger on the training dataset. Refer to `help(nltk.tag.hmm.HiddenMarkovModelTagger.train)` if necessary.\n",
    "2. Use the trained model to tag the sentence\n",
    "3. Use the trained model to evaluate the tagger on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_trainer(sentence, size):\n",
    "    \"\"\" Create an HMM tagger from the Brown news corpus, and test it by tagging a sample sentence\n",
    "    \n",
    "    :param sentence: An untagged sentence as an example\n",
    "    :type sentence: list(str)\n",
    "    :param size: Number of sentences to train on (be sure to leave room for the test data)\n",
    "    :type size: int\n",
    "    :return: The tagger, the sample sentence with tags, entropy of model wrt 100 test sentences\n",
    "    :rtype: tuple(nltk.tag.hmm.HiddenMarkovModelTagger, list(tuple(str,str)), float)\"\"\"\n",
    "    tagged_sentences = brown.tagged_sents(categories='news')\n",
    "\n",
    "    # set up the training data\n",
    "    train_data = tagged_sentences[-size:]\n",
    "\n",
    "    # set up the test data\n",
    "    test_data = tagged_sentences[:100]\n",
    "    \n",
    "    # Hint: use help on HiddenMarkovModelTagger to find out how to train, tag and evaluate an HMM tagger\n",
    "\n",
    "    # TODO: train a HiddenMarkovModelTagger, using the train() method\n",
    "    tagger = HiddenMarkovModelTagger.train(train_data)\n",
    "\n",
    "    # TODO: using the hmm tagger tag the sentence\n",
    "    hmm_tagged_sentence = tagger.tag(sentence)\n",
    "\n",
    "    # TODO: using the hmm tagger, evaluate accuracy score on the test data\n",
    "    acc = tagger.evaluate(test_data)\n",
    "\n",
    "    return tagger, hmm_tagged_sentence, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training nltk.HiddenMarkovModelTagge with 500 sentences...\n",
      "\tSentence tagged with model:\n",
      "[   ('The', 'AT'),\n",
      "    (\"mayor's\", 'QL'),\n",
      "    ('present', 'JJ'),\n",
      "    ('term', 'NN'),\n",
      "    ('of', 'IN'),\n",
      "    ('office', 'NN'),\n",
      "    ('expires', '.'),\n",
      "    ('Jan.', '.'),\n",
      "    ('1', '.'),\n",
      "    ('.', '.')]\n",
      "\tAccuracy score on the test set: 71.4727%\n",
      "\n",
      "Training nltk.HiddenMarkovModelTagge with 3000 sentences...\n",
      "\tSentence tagged with model:\n",
      "[   ('The', 'AT'),\n",
      "    (\"mayor's\", 'NN$'),\n",
      "    ('present', 'JJ'),\n",
      "    ('term', 'NN'),\n",
      "    ('of', 'IN'),\n",
      "    ('office', 'NN'),\n",
      "    ('expires', 'IN'),\n",
      "    ('Jan.', 'NP'),\n",
      "    ('1', 'CD'),\n",
      "    ('.', '.')]\n",
      "\tAccuracy score on the test set: 86.8607%\n"
     ]
    }
   ],
   "source": [
    "def test_my_trainer():\n",
    "    tagged_sentences = brown.tagged_sents(categories='news')\n",
    "    words = [tp[0] for tp in tagged_sentences[42]]\n",
    "    tagger, hmm_tagged_sentence, acc = my_trainer(words, 500)\n",
    "    print('Training nltk.HiddenMarkovModelTagge with 500 sentences...')\n",
    "    print('\\tSentence tagged with model:')\n",
    "    pp.pprint(hmm_tagged_sentence)\n",
    "    print('\\tAccuracy score on the test set: %.4f%%' % (100.0*acc))\n",
    "    print()\n",
    "\n",
    "    tagger, hmm_tagged_sentence, acc = my_trainer(words, 3000)\n",
    "    print('Training nltk.HiddenMarkovModelTagge with 3000 sentences...')\n",
    "    print('\\tSentence tagged with model:')\n",
    "    pp.pprint(hmm_tagged_sentence)\n",
    "    print('\\tAccuracy score on the test set: %.4f%%' % (100.0*acc))\n",
    "\n",
    "test_my_trainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the tagged sentence and the accuracy of the tagger. How does the size of the training set affect the accuracy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Transition and Emission Probabilities\n",
    "\n",
    "In the previous exercise we learned how to train and evaluate an HMM tagger.\n",
    "We have used the HMM tagger as a black box and have seen how the training\n",
    "data affects the accuracy of the tagger. In order to get a better understanding\n",
    "of the HMM we will look at the two components of this model:\n",
    "    \n",
    "* The transition model\n",
    "* The emission model\n",
    "\n",
    "The transition model estimates $P (tag_{i+1} |tag_i )$, the probability of a POS tag\n",
    "at position $i+1$ given the previous tag (at position $i$). The emission model\n",
    "estimates $P (word|tag)$, the probability of the observed word given a tag.\n",
    "\n",
    "Given the above definitions, we will need to learn a Conditional Probability\n",
    "Distribution for each of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ConditionalProbDist in module nltk.probability:\n",
      "\n",
      "class ConditionalProbDist(ConditionalProbDistI)\n",
      " |  A conditional probability distribution modeling the experiments\n",
      " |  that were used to generate a conditional frequency distribution.\n",
      " |  A ConditionalProbDist is constructed from a\n",
      " |  ``ConditionalFreqDist`` and a ``ProbDist`` factory:\n",
      " |  \n",
      " |  - The ``ConditionalFreqDist`` specifies the frequency\n",
      " |    distribution for each condition.\n",
      " |  - The ``ProbDist`` factory is a function that takes a\n",
      " |    condition's frequency distribution, and returns its\n",
      " |    probability distribution.  A ``ProbDist`` class's name (such as\n",
      " |    ``MLEProbDist`` or ``HeldoutProbDist``) can be used to specify\n",
      " |    that class's constructor.\n",
      " |  \n",
      " |  The first argument to the ``ProbDist`` factory is the frequency\n",
      " |  distribution that it should model; and the remaining arguments are\n",
      " |  specified by the ``factory_args`` parameter to the\n",
      " |  ``ConditionalProbDist`` constructor.  For example, the following\n",
      " |  code constructs a ``ConditionalProbDist``, where the probability\n",
      " |  distribution for each condition is an ``ELEProbDist`` with 10 bins:\n",
      " |  \n",
      " |      >>> from nltk.corpus import brown\n",
      " |      >>> from nltk.probability import ConditionalFreqDist\n",
      " |      >>> from nltk.probability import ConditionalProbDist, ELEProbDist\n",
      " |      >>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])\n",
      " |      >>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)\n",
      " |      >>> cpdist['passed'].max()\n",
      " |      'VBD'\n",
      " |      >>> cpdist['passed'].prob('VBD')\n",
      " |      0.423...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ConditionalProbDist\n",
      " |      ConditionalProbDistI\n",
      " |      builtins.dict\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, cfdist, probdist_factory, *factory_args, **factory_kw_args)\n",
      " |      Construct a new conditional probability distribution, based on\n",
      " |      the given conditional frequency distribution and ``ProbDist``\n",
      " |      factory.\n",
      " |      \n",
      " |      :type cfdist: ConditionalFreqDist\n",
      " |      :param cfdist: The ``ConditionalFreqDist`` specifying the\n",
      " |          frequency distribution for each condition.\n",
      " |      :type probdist_factory: class or function\n",
      " |      :param probdist_factory: The function or class that maps\n",
      " |          a condition's frequency distribution to its probability\n",
      " |          distribution.  The function is called with the frequency\n",
      " |          distribution as its first argument,\n",
      " |          ``factory_args`` as its remaining arguments, and\n",
      " |          ``factory_kw_args`` as keyword arguments.\n",
      " |      :type factory_args: (any)\n",
      " |      :param factory_args: Extra arguments for ``probdist_factory``.\n",
      " |          These arguments are usually used to specify extra\n",
      " |          properties for the probability distributions of individual\n",
      " |          conditions, such as the number of bins they contain.\n",
      " |      :type factory_kw_args: (any)\n",
      " |      :param factory_kw_args: Extra keyword arguments for ``probdist_factory``.\n",
      " |  \n",
      " |  __missing__(self, key)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ConditionalProbDistI:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return a string representation of this ``ConditionalProbDist``.\n",
      " |      \n",
      " |      :rtype: str\n",
      " |  \n",
      " |  __unicode__ = __str__(self, /)\n",
      " |  \n",
      " |  conditions(self)\n",
      " |      Return a list of the conditions that are represented by\n",
      " |      this ``ConditionalProbDist``.  Use the indexing operator to\n",
      " |      access the probability distribution for a given condition.\n",
      " |      \n",
      " |      :rtype: list\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from ConditionalProbDistI:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.dict:\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      True if D has a key k, else False.\n",
      " |  \n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      D.__sizeof__() -> size of D in memory, in bytes\n",
      " |  \n",
      " |  clear(...)\n",
      " |      D.clear() -> None.  Remove all items from D.\n",
      " |  \n",
      " |  copy(...)\n",
      " |      D.copy() -> a shallow copy of D\n",
      " |  \n",
      " |  fromkeys(iterable, value=None, /) from abc.ABCMeta\n",
      " |      Returns a new dict with keys from iterable and values equal to value.\n",
      " |  \n",
      " |  get(...)\n",
      " |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
      " |  \n",
      " |  items(...)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  keys(...)\n",
      " |      D.keys() -> a set-like object providing a view on D's keys\n",
      " |  \n",
      " |  pop(...)\n",
      " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      " |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      " |  \n",
      " |  popitem(...)\n",
      " |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
      " |      2-tuple; but raise KeyError if D is empty.\n",
      " |  \n",
      " |  setdefault(...)\n",
      " |      D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n",
      " |  \n",
      " |  update(...)\n",
      " |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      " |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      " |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      " |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      " |  \n",
      " |  values(...)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from builtins.dict:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.probability.ConditionalProbDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Emission Model\n",
    "\n",
    "In this exercise we will estimate the emission model. In order to compute the\n",
    "Conditional Probability Distribution of $P (word|tag)$ we first have to compute\n",
    "the Conditional Frequency Distribution of a word given a tag.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ConditionalFreqDist in module nltk.probability:\n",
      "\n",
      "class ConditionalFreqDist(collections.defaultdict)\n",
      " |  A collection of frequency distributions for a single experiment\n",
      " |  run under different conditions.  Conditional frequency\n",
      " |  distributions are used to record the number of times each sample\n",
      " |  occurred, given the condition under which the experiment was run.\n",
      " |  For example, a conditional frequency distribution could be used to\n",
      " |  record the frequency of each word (type) in a document, given its\n",
      " |  length.  Formally, a conditional frequency distribution can be\n",
      " |  defined as a function that maps from each condition to the\n",
      " |  FreqDist for the experiment under that condition.\n",
      " |  \n",
      " |  Conditional frequency distributions are typically constructed by\n",
      " |  repeatedly running an experiment under a variety of conditions,\n",
      " |  and incrementing the sample outcome counts for the appropriate\n",
      " |  conditions.  For example, the following code will produce a\n",
      " |  conditional frequency distribution that encodes how often each\n",
      " |  word type occurs, given the length of that word type:\n",
      " |  \n",
      " |      >>> from nltk.probability import ConditionalFreqDist\n",
      " |      >>> from nltk.tokenize import word_tokenize\n",
      " |      >>> sent = \"the the the dog dog some other words that we do not care about\"\n",
      " |      >>> cfdist = ConditionalFreqDist()\n",
      " |      >>> for word in word_tokenize(sent):\n",
      " |      ...     condition = len(word)\n",
      " |      ...     cfdist[condition][word] += 1\n",
      " |  \n",
      " |  An equivalent way to do this is with the initializer:\n",
      " |  \n",
      " |      >>> cfdist = ConditionalFreqDist((len(word), word) for word in word_tokenize(sent))\n",
      " |  \n",
      " |  The frequency distribution for each condition is accessed using\n",
      " |  the indexing operator:\n",
      " |  \n",
      " |      >>> cfdist[3]\n",
      " |      FreqDist({'the': 3, 'dog': 2, 'not': 1})\n",
      " |      >>> cfdist[3].freq('the')\n",
      " |      0.5\n",
      " |      >>> cfdist[3]['dog']\n",
      " |      2\n",
      " |  \n",
      " |  When the indexing operator is used to access the frequency\n",
      " |  distribution for a condition that has not been accessed before,\n",
      " |  ``ConditionalFreqDist`` creates a new empty FreqDist for that\n",
      " |  condition.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ConditionalFreqDist\n",
      " |      collections.defaultdict\n",
      " |      builtins.dict\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  N(self)\n",
      " |      Return the total number of sample outcomes that have been\n",
      " |      recorded by this ``ConditionalFreqDist``.\n",
      " |      \n",
      " |      :rtype: int\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      Add counts from two ConditionalFreqDists.\n",
      " |  \n",
      " |  __and__(self, other)\n",
      " |      Intersection is the minimum of corresponding counts.\n",
      " |  \n",
      " |  __ge__(self, other)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __gt__(self, other)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __init__(self, cond_samples=None)\n",
      " |      Construct a new empty conditional frequency distribution.  In\n",
      " |      particular, the count for every sample, under every condition,\n",
      " |      is zero.\n",
      " |      \n",
      " |      :param cond_samples: The samples to initialize the conditional\n",
      " |          frequency distribution with\n",
      " |      :type cond_samples: Sequence of (condition, sample) tuples\n",
      " |  \n",
      " |  __le__(self, other)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __lt__(self, other)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __or__(self, other)\n",
      " |      Union is the maximum of value in either of the input counters.\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      Return state information for pickling.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return a string representation of this ``ConditionalFreqDist``.\n",
      " |      \n",
      " |      :rtype: str\n",
      " |  \n",
      " |  __sub__(self, other)\n",
      " |      Subtract count, but keep only results with positive counts.\n",
      " |  \n",
      " |  __unicode__ = __str__(self, /)\n",
      " |  \n",
      " |  conditions(self)\n",
      " |      Return a list of the conditions that have been accessed for\n",
      " |      this ``ConditionalFreqDist``.  Use the indexing operator to\n",
      " |      access the frequency distribution for a given condition.\n",
      " |      Note that the frequency distributions for some conditions\n",
      " |      may contain zero sample outcomes.\n",
      " |      \n",
      " |      :rtype: list\n",
      " |  \n",
      " |  plot(self, *args, **kwargs)\n",
      " |      Plot the given samples from the conditional frequency distribution.\n",
      " |      For a cumulative plot, specify cumulative=True.\n",
      " |      (Requires Matplotlib to be installed.)\n",
      " |      \n",
      " |      :param samples: The samples to plot\n",
      " |      :type samples: list\n",
      " |      :param title: The title for the graph\n",
      " |      :type title: str\n",
      " |      :param conditions: The conditions to plot (default is all)\n",
      " |      :type conditions: list\n",
      " |  \n",
      " |  tabulate(self, *args, **kwargs)\n",
      " |      Tabulate the given samples from the conditional frequency distribution.\n",
      " |      \n",
      " |      :param samples: The samples to plot\n",
      " |      :type samples: list\n",
      " |      :param conditions: The conditions to plot (default is all)\n",
      " |      :type conditions: list\n",
      " |      :param cumulative: A flag to specify whether the freqs are cumulative (default = False)\n",
      " |      :type title: bool\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from collections.defaultdict:\n",
      " |  \n",
      " |  __copy__(...)\n",
      " |      D.copy() -> a shallow copy of D.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __missing__(...)\n",
      " |      __missing__(key) # Called by __getitem__ for missing key; pseudo-code:\n",
      " |      if self.default_factory is None: raise KeyError((key,))\n",
      " |      self[key] = value = self.default_factory()\n",
      " |      return value\n",
      " |  \n",
      " |  copy(...)\n",
      " |      D.copy() -> a shallow copy of D.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from collections.defaultdict:\n",
      " |  \n",
      " |  default_factory\n",
      " |      Factory for default value called by __missing__().\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.dict:\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      True if D has a key k, else False.\n",
      " |  \n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      D.__sizeof__() -> size of D in memory, in bytes\n",
      " |  \n",
      " |  clear(...)\n",
      " |      D.clear() -> None.  Remove all items from D.\n",
      " |  \n",
      " |  fromkeys(iterable, value=None, /) from builtins.type\n",
      " |      Returns a new dict with keys from iterable and values equal to value.\n",
      " |  \n",
      " |  get(...)\n",
      " |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
      " |  \n",
      " |  items(...)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  keys(...)\n",
      " |      D.keys() -> a set-like object providing a view on D's keys\n",
      " |  \n",
      " |  pop(...)\n",
      " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      " |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      " |  \n",
      " |  popitem(...)\n",
      " |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
      " |      2-tuple; but raise KeyError if D is empty.\n",
      " |  \n",
      " |  setdefault(...)\n",
      " |      D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n",
      " |  \n",
      " |  update(...)\n",
      " |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      " |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      " |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      " |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      " |  \n",
      " |  values(...)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from builtins.dict:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.probability.ConditionalFreqDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructor of the ConditionalFreqDist class takes as input a list of tuples,\n",
    "each tuple consisting of a condition and an observation. For the emission model,\n",
    "the conditions are tags and the observations are the words. The template of the\n",
    "function that you have to implement takes as argument the list of tagged words\n",
    "from the Brown corpus.\n",
    "\n",
    "1. Build the dataset to be passed to the `ConditionalFreqDist()` constructor. Words should be lowercased. Each item of data should be a tuple of tag (a condition) and word (an observation).\n",
    "2. Compute the Conditional Frequency Distribution of words given tags.\n",
    "3. Return the top 10 most frequent words given the tag NN.\n",
    "4. Compute the Conditional Probability Distribution for the above Conditional Frequency Distribution. Use the `MLEProbDist` estimator when calling the ConditionalProbDist constructor.\n",
    "5. Compute the probabilities:\n",
    "\n",
    " $P(\\text{year}|\\text{NN})$ \n",
    " \n",
    " $P(\\text{year}|\\text{DT})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_emission_model(tagged_words):\n",
    "    \"\"\"Build and sample Conditional{Freq->Prob}Dist for word given tag from a list of tagged words using MLE\n",
    "    \n",
    "    :param tagged_words: tagged words (word,tag)\n",
    "    :type tagged_words: list(tuple(str,str))\n",
    "    :return: Conditional Freq dist of word given tag, top 10 words with tag NN,\n",
    "             Conditional Prob dist of word given tag, P('year'|'NN'), P('year'|'DT')\n",
    "    :rtype: tuple(nltk.probability.ConditionalFreqDist,list(tuple(str,int)),nltk.probability.ConditionalProbDist,float,float)\"\"\"\n",
    "    \n",
    "    # in the previous labs we've seen how to build a freq dist\n",
    "    # we need conditional distributions to estimate the transition and emission models\n",
    "    # in this exercise we estimate the emission model\n",
    "    \n",
    "    # TODO: prepare the data\n",
    "    # the data object should be a list of tuples of conditions and observations\n",
    "    # in our case the tuples should be of the form (tag,word) where words are lowercased\n",
    "    data = [ (tag, word.lower()) for (word, tag) in tagged_words]\n",
    "\n",
    "    # TODO: compute a Conditional Frequency Distribution for words given their tags using our data\n",
    "    emission_FD =ConditionalFreqDist(data)\n",
    "\n",
    "    # TODO: find the top 10 most frequent words given the tag NN\n",
    "    top_NN = emission_FD['NN'].most_common(10)\n",
    "\n",
    "    # TODO: Compute the Conditional Probability Distribution using the above Conditional Frequency Distribution. \n",
    "    #       Use nltk.probability.MLEProbDist estimator.\n",
    "    emission_PD =ConditionalProbDist(emission_FD,MLEProbDist)\n",
    "\n",
    "    # TODO: compute the probabilities of P(year|NN) and P(year|DT)\n",
    "    p_NN = emission_PD['NN'].prob('year')\n",
    "    p_DT = emission_PD['DT'].prob('year')\n",
    "\n",
    "    return emission_FD, top_NN, emission_PD, p_NN, p_DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of words given the tag *NN*: \n",
      "[   ('year', 137),\n",
      "    ('time', 98),\n",
      "    ('state', 92),\n",
      "    ('week', 86),\n",
      "    ('man', 72),\n",
      "    ('home', 72),\n",
      "    ('program', 65),\n",
      "    ('school', 65),\n",
      "    ('night', 64),\n",
      "    ('day', 62)]\n",
      "P(year|NN) =  0.0104087524692296\n",
      "P(year|DT) =  0.0\n"
     ]
    }
   ],
   "source": [
    "def test_emission_model():\n",
    "    tagged_words = brown.tagged_words(categories='news')\n",
    "    (emission_FD, top_NN, emission_PD, p_NN, p_DT) = my_emission_model(tagged_words)\n",
    "    print('Frequency of words given the tag *NN*: ')\n",
    "    pp.pprint(top_NN)\n",
    "    print('P(year|NN) = ', p_NN)\n",
    "    print('P(year|DT) = ', p_DT)\n",
    "\n",
    "test_emission_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the estimated probabilities. Why is P(year|DT) = 0 ? \n",
    "\n",
    "What are the problems with having zero (0) probabilities and what can be done to\n",
    "avoid this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Transition Model\n",
    "\n",
    "In this exercise we will estimate the transition model. In order to compute the\n",
    "Conditional Probability Distribution of $P (tag_{i+1} |tag_i )$ we first have to compute\n",
    "the Conditional Frequency Distribution of a tag at position $i + 1$ given the previous tag.\n",
    "\n",
    "The constructor of the `ConditionalFreqDist` class takes as input a list of tuples, each tuple consisting of a condition and an observation. For the transition\n",
    "model, the conditions are tags at position i and the observations are tags at\n",
    "position $i + 1$. The template of the function that you have to implement takes\n",
    "as argument the list of tagged sentences from the Brown corpus.\n",
    "\n",
    "1. Build the dataset to be passed to the `ConditionalFreqDist()` constructor. Each item in your data should be a pair of condition and observation: $(tag_i,tag_{i+1})$\n",
    "2. Compute the Conditional Frequency Distribution of a tag at position $i + 1$ given the previous tag.\n",
    "3. Compute the Conditional Probability Distribution for the above Conditional Frequency Distribution. Use the `MLEProbDist` estimator when calling the `ConditionalProbDist` constructor.\n",
    "4. Compute the probabilities \n",
    "   \n",
    "   $P(\\text{NN}|\\text{VBD})$ \n",
    "   \n",
    "   $P(\\text{NN}|\\text{DT})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_transition_model(tagged_sentences):\n",
    "    \"\"\"Build and sample Conditional{Freq->Prob}Dist for tag given preceding tag from a list of tagged words using MLE\n",
    "    \n",
    "    :param tagged_sentences: Tagged sentences for training and testing\n",
    "    :type tagged_sentences: list(list(tuple(str,str)))\n",
    "    :return: Conditional Freq dist of tag given preceding tag,\n",
    "             Conditional Prob dist of tag given preceding tag, P('NN'|'VBD') and P('NN'|'DT')\n",
    "    :rtype: tuple(nltk.probability.ConditionalFreqDist,nltk.probability.ConditionalProbDist,float,float)\"\"\"\n",
    "    \n",
    "    # TODO: prepare the data\n",
    "    # the data object should be an array of tuples of conditions and observations\n",
    "    # in our case the tuples will be of the form (tag_(i),tag_(i+1))\n",
    "    tagGenerators=(((s[i][1],s[i+1][1]) for i in range(len(s)-1)) for s in tagged_sentences)\n",
    "    # tagGenerators is an iterator of iterators of pairs of tags\n",
    "    # The following chains them all together to produce an iterator of pairs of tags\n",
    "    data = itertools.chain.from_iterable(tagGenerators)\n",
    "\n",
    "    # TODO: compute a Conditional Frequency Distribution for a tag given the previous tag\n",
    "    transition_FD =ConditionalFreqDist(data)\n",
    "\n",
    "    # TODO: compute a Conditional Probability Distribution for the\n",
    "    # transition probability P(tag_(i+1)|tag_(i)) using an MLEProbDist\n",
    "    # to estimate the probabilities\n",
    "    transition_PD =ConditionalProbDist(transition_FD, MLEProbDist)\n",
    "\n",
    "    # TODO: compute the probabilities of P('NN'|'VBD') and P('NN'|'DT')\n",
    "    p_VBD_NN = transition_PD['VBD'].prob('NN')\n",
    "    p_DT_NN = transition_PD['DT'].prob('NN')\n",
    "\n",
    "    return transition_FD, transition_PD, p_VBD_NN, p_DT_NN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transition_model():\n",
    "    tagged_sentences = brown.tagged_sents(categories='news')\n",
    "    (transition_FD, transition_PD, p_VBD_NN, p_DT_NN) = my_transition_model(tagged_sentences)\n",
    "    print('P(NN|VBD) = ', p_VBD_NN)\n",
    "    print('P(NN|DT) = ', p_DT_NN)\n",
    "    \n",
    "test_transition_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the results what you would expect? The sequence NN DT seems very probable. \n",
    "\n",
    "How will this affect the sequence tagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going further\n",
    "\n",
    "Modify your code for exercise 3 to use a different estimator, to introduce some\n",
    "smoothing, and compare the results with the original.\n",
    "In exercise 4 we didn’t do anything about the boundaries. Modify your code for\n",
    "exercise 4 to use `<s>` at the beginning of every sentence and `</s>` at the end.\n",
    "\n",
    "Explore the resulting conditional probabilities. What is the most likely tag at\n",
    "the beginning of a sentence? At the end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
