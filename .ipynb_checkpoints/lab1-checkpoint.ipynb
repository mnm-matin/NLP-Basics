{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre style=\"float: right\">version 1.0.1</pre>\n",
    "# FNLP: Lab Session 1: Corpora and Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim\n",
    "\n",
    "The aims of this lab session are to \n",
    "1. explore the different uses of language in different documents, authored by different people and \n",
    "2. introduce the construction of language models using Python’s Natural Language Tool Kit (NLTK).\n",
    "\n",
    "This year labs are run through Jupyter Notebooks. Successful completion of this lab is important as the first assignment for FNLP builds on some of the concepts and methods that are introduced here. By the end of this lab session, you should be able to:\n",
    "\n",
    "* Access the corpora provided in NLTK\n",
    "* Compute a frequency distribution\n",
    "* Train a language model\n",
    "* Use a language model to compute bigram probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Help \n",
    "\n",
    "Python contains a built-in help module that runs in an interactive mode. To\n",
    "run the interactive help, type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`help()` will run until interrupted. If a cell is running it will block any other cell from running until it has completed. You can check if a cell is still running by looking at `In [*]:` to the left of any cell. If there is a `*` inside the brackets the cell is still running. As soon as the cell has stopped running the `*` will be replaced by a number. \n",
    "\n",
    "**Before moving on** you will need to interrupt `help()` (make it stop running). To interrupt running cells go to **`kernel/interrupt`** at the top of the webpage. You can also hit the **big black square button** right underneath (if you hover over it it will say interrupt kernel). This is equivalent to hitting CTRL-d to interrupt a running program in the terminal or the python shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you know the name of the module that you want to get help on, type:\n",
    "`import <module_name>`\n",
    "`help(<module_name>)`\n",
    "try looking at the help documentation for `matplotlib.pyplot` - a python package introduced in the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "help(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you know the name of the module and the method that you want to get help\n",
    "on, type `help(<module_name>.<method_name>)` (note you must have imported `<module_name>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The FNLP lab sessions will make use of the Natural Language Tool Kit (NLTK) for Python. NLTK is a platform for writing programs to process human language data, that provides both corpora and modules. For more information on NLTK, please visit http://www.nltk.org/.\n",
    "\n",
    "For each exercise, edit the corresponding function in the notebook, then run the lines which prepare for and invoke that function.\n",
    "\n",
    "Let's start by importing NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Corpora\n",
    "\n",
    "NLTK provides many corpora and covers many genres of text. Some of the\n",
    "corpora are listed below:\n",
    "\n",
    "* Gutenberg: out of copyright books\n",
    "* Brown: a general corpus of texts including novels, short stories and news\n",
    "articles\n",
    "* Inaugural: U.S. Presidential inaugural speeches\n",
    "\n",
    "To see a complete list of available corpora you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'brown', 'qc', 'shakespeare', 'sentence_polarity', 'inaugural', 'chat80', 'brown_tei', 'swadesh', 'mte_teip5', 'subjectivity', 'floresta', 'ptb', 'pl196x', 'cess_esp', 'lin_thesaurus', 'genesis', 'state_union', 'names', 'pros_cons', 'toolbox', 'product_reviews_1', 'webtext', 'conll2000', 'ycoe', 'comparative_sentences', 'nps_chat', 'wordnet_ic', 'alpino', 'mac_morpho', 'framenet_v15', 'unicode_samples', 'switchboard', 'verbnet', 'treebank', 'gutenberg', 'omw', 'problem_reports', 'gazetteers', 'udhr2', 'biocreative_ppi', 'paradigms', 'indian', 'words', 'udhr', 'city_database', 'wordnet', 'rte', 'dependency_treebank', 'timit', 'conll2002', 'cmudict', 'ieer', 'sentiwordnet', 'twitter_samples', 'crubadan', 'cess_cat', 'sinica_treebank', 'stopwords', 'pil', 'europarl_raw', 'opinion_lexicon', 'product_reviews_2', 'movie_reviews', 'senseval', 'smultron', 'ppattach', 'kimmo']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(nltk.data.find('corpora')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each corpus contains a number of texts. We’ll work with the inaugural corpus, and explore what the corpus contains. Make sure you have imported the nltk module first and then load the inaugural corpus by typing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To list all of the documents in the inaugural corpus, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', '1801-Jefferson.txt', '1805-Jefferson.txt', '1809-Madison.txt', '1813-Madison.txt', '1817-Monroe.txt', '1821-Monroe.txt', '1825-Adams.txt', '1829-Jackson.txt', '1833-Jackson.txt', '1837-VanBuren.txt', '1841-Harrison.txt', '1845-Polk.txt', '1849-Taylor.txt', '1853-Pierce.txt', '1857-Buchanan.txt', '1861-Lincoln.txt', '1865-Lincoln.txt', '1869-Grant.txt', '1873-Grant.txt', '1877-Hayes.txt', '1881-Garfield.txt', '1885-Cleveland.txt', '1889-Harrison.txt', '1893-Cleveland.txt', '1897-McKinley.txt', '1901-McKinley.txt', '1905-Roosevelt.txt', '1909-Taft.txt', '1913-Wilson.txt', '1917-Wilson.txt', '1921-Harding.txt', '1925-Coolidge.txt', '1929-Hoover.txt', '1933-Roosevelt.txt', '1937-Roosevelt.txt', '1941-Roosevelt.txt', '1945-Roosevelt.txt', '1949-Truman.txt', '1953-Eisenhower.txt', '1957-Eisenhower.txt', '1961-Kennedy.txt', '1965-Johnson.txt', '1969-Nixon.txt', '1973-Nixon.txt', '1977-Carter.txt', '1981-Reagan.txt', '1985-Reagan.txt', '1989-Bush.txt', '1993-Clinton.txt', '1997-Clinton.txt', '2001-Bush.txt', '2005-Bush.txt', '2009-Obama.txt']\n"
     ]
    }
   ],
   "source": [
    "print(inaugural.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on we’ll work with President Barack Obama’s inaugural speech from 2009 (2009-Obama.txt). The contents of each document (in a corpus) may be accessed via a number of corpus readers. The plaintext corpus reader provides methods to view the raw text (raw), a list of words (words) or a list of sentences: to list all of the documents in the inaugural corpus, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My fellow citizens:\n",
      "\n",
      "I stand here today humbled by the task before us, grateful for the trust you have bestowed, mindful of the sacrifices borne by our ancestors. I thank President Bush for his service to our nation, as well as the generosity and cooperation he has shown throughout this transition.\n",
      "\n",
      "Forty-four Americans have now taken the presidential oath. The words have been spoken during rising tides of prosperity and the still waters of peace. Yet, every so often the oath is taken amidst gathering clouds and raging storms. At these moments, America has carried on not simply because of the skill or vision of those in high office, but because We the People have remained faithful to the ideals of our forbearers, and true to our founding documents.\n",
      "\n",
      "So it has been. So it must be with this generation of Americans.\n",
      "\n",
      "That we are in the midst of crisis is now well understood. Our nation is at war, against a far-reaching network of violence and hatred. Our economy is badly weakened, a consequence of greed and irresponsibility on the part of some, but also our collective failure to make hard choices and prepare the nation for a new age. Homes have been lost; jobs shed; businesses shuttered. Our health care is too costly; our schools fail too many; and each day brings further evidence that the ways we use energy strengthen our adversaries and threaten our planet.\n",
      "\n",
      "These are the indicators of crisis, subject to data and statistics. Less measurable but no less profound is a sapping of confidence across our land -- a nagging fear that America's decline is inevitable, that the next generation must lower its sights.\n",
      "\n",
      "Today I say to you that the challenges we face are real. They are serious and they are many. They will not be met easily or in a short span of time. But know this, America -- they will be met.\n",
      "\n",
      "On this day, we gather because we have chosen hope over fear, unity of purpose over conflict and discord.\n",
      "\n",
      "On this day, we come to proclaim an end to the petty grievances and false promises, the recriminations and worn-out dogmas that for far too long have strangled our politics.\n",
      "\n",
      "We remain a young nation, but in the words of Scripture, the time has come to set aside childish things. The time has come to reaffirm our enduring spirit; to choose our better history; to carry forward that precious gift, that noble idea, passed on from generation to generation: the God-given promise that all are equal, all are free, and all deserve a chance to pursue their full measure of happiness.\n",
      "\n",
      "In reaffirming the greatness of our nation, we understand that greatness is never a given. It must be earned. Our journey has never been one of shortcuts or settling for less. It has not been the path for the faint-hearted -- for those who prefer leisure over work, or seek only the pleasures of riches and fame. Rather, it has been the risk-takers, the doers, the makers of things'some celebrated but more often men and women obscure in their labor, who have carried us up the long, rugged path towards prosperity and freedom.\n",
      "\n",
      "For us, they packed up their few worldly possessions and traveled across oceans in search of a new life.\n",
      "\n",
      "For us, they toiled in sweatshops and settled the West; endured the lash of the whip and plowed the hard earth.\n",
      "\n",
      "For us, they fought and died, in places like Concord and Gettysburg; Normandy and Khe Sahn.\n",
      "\n",
      "Time and again these men and women struggled and sacrificed and worked till their hands were raw so that we might live a better life. They saw America as bigger than the sum of our individual ambitions; greater than all the differences of birth or wealth or faction.\n",
      "\n",
      "This is the journey we continue today. We remain the most prosperous, powerful nation on Earth. Our workers are no less productive than when this crisis began. Our minds are no less inventive, our goods and services no less needed than they were last week or last month or last year. Our capacity remains undiminished. But our time of standing pat, of protecting narrow interests and putting off unpleasant decisions -- that time has surely passed. Starting today, we must pick ourselves up, dust ourselves off, and begin again the work of remaking America.\n",
      "\n",
      "For everywhere we look, there is work to be done. The state of our economy calls for action, bold and swift, and we will act -- not only to create new jobs, but to lay a new foundation for growth. We will build the roads and bridges, the electric grids and digital lines that feed our commerce and bind us together. We will restore science to its rightful place, and wield technology's wonders to raise health care's quality and lower its cost. We will harness the sun and the winds and the soil to fuel our cars and run our factories. And we will transform our schools and colleges and universities to meet the demands of a new age. All this we can do. All this we will do.\n",
      "\n",
      "Now, there are some who question the scale of our ambitions -- who suggest that our system cannot tolerate too many big plans. Their memories are short. For they have forgotten what this country has already done; what free men and women can achieve when imagination is joined to common purpose, and necessity to courage.\n",
      "\n",
      "What the cynics fail to understand is that the ground has shifted beneath them -- that the stale political arguments that have consumed us for so long no longer apply. The question we ask today is not whether our government is too big or too small, but whether it works -- whether it helps families find jobs at a decent wage, care they can afford, a retirement that is dignified. Where the answer is yes, we intend to move forward. Where the answer is no, programs will end. And those of us who manage the public's dollars will be held to account -- to spend wisely, reform bad habits, and do our business in the light of day -- because only then can we restore the vital trust between a people and their government.\n",
      "\n",
      "Nor is the question before us whether the market is a force for good or ill. Its power to generate wealth and expand freedom is unmatched, but this crisis has reminded us that without a watchful eye, the market can spin out of control -- the nation cannot prosper long when it favors only the prosperous. The success of our economy has always depended not just on the size of our Gross Domestic Product, but on the reach of our prosperity; on the ability to extend opportunity to every willing heart -- not out of charity, but because it is the surest route to our common good.\n",
      "\n",
      "As for our common defense, we reject as false the choice between our safety and our ideals. Our Founding Fathers, faced with perils that we can scarcely imagine, drafted a charter to assure the rule of law and the rights of man, a charter expanded by the blood of generations. Those ideals still light the world, and we will not give them up for expedience's sake. And so to all the other peoples and governments who are watching today, from the grandest capitals to the small village where my father was born: know that America is a friend of each nation and every man, woman, and child who seeks a future of peace and dignity, and we are ready to lead once more.\n",
      "\n",
      "Recall that earlier generations faced down fascism and communism not just with missiles and tanks, but with the sturdy alliances and enduring convictions. They understood that our power alone cannot protect us, nor does it entitle us to do as we please. Instead, they knew that our power grows through its prudent use; our security emanates from the justness of our cause, the force of our example, the tempering qualities of humility and restraint.\n",
      "\n",
      "We are the keepers of this legacy. Guided by these principles once more, we can meet those new threats that demand even greater effort -- even greater cooperation and understanding between nations. We will begin to responsibly leave Iraq to its people, and forge a hard-earned peace in Afghanistan. With old friends and former foes, we will work tirelessly to lessen the nuclear threat, and roll back the specter of a warming planet. We will not apologize for our way of life, nor will we waver in its defense, and for those who seek to advance their aims by inducing terror and slaughtering innocents, we say to you now that our spirit is stronger and cannot be broken; you cannot outlast us, and we will defeat you.\n",
      "\n",
      "For we know that our patchwork heritage is a strength, not a weakness. We are a nation of Christians and Muslims, Jews and Hindus -- and non-believers. We are shaped by every language and culture, drawn from every end of this Earth; and because we have tasted the bitter swill of civil war and segregation, and emerged from that dark chapter stronger and more united, we cannot help but believe that the old hatreds shall someday pass; that the lines of tribe shall soon dissolve; that as the world grows smaller, our common humanity shall reveal itself; and that America must play its role in ushering in a new era of peace.\n",
      "\n",
      "To the Muslim world, we seek a new way forward, based on mutual interest and mutual respect. To those leaders around the globe who seek to sow conflict, or blame their society's ills on the West -- know that your people will judge you on what you can build, not what you destroy. To those who cling to power through corruption and deceit and the silencing of dissent, know that you are on the wrong side of history; but that we will extend a hand if you are willing to unclench your fist.\n",
      "\n",
      "To the people of poor nations, we pledge to work alongside you to make your farms flourish and let clean waters flow; to nourish starved bodies and feed hungry minds. And to those nations like ours that enjoy relative plenty, we say we can no longer afford indifference to the suffering outside our borders; nor can we consume the world's resources without regard to effect. For the world has changed, and we must change with it.\n",
      "\n",
      "As we consider the road that unfolds before us, we remember with humble gratitude those brave Americans who, at this very hour, patrol far-off deserts and distant mountains. They have something to tell us, just as the fallen heroes who lie in Arlington whisper through the ages. We honor them not only because they are the guardians of our liberty, but because they embody the spirit of service; a willingness to find meaning in something greater than themselves. And yet, at this moment -- a moment that will define a generation -- it is precisely this spirit that must inhabit us all.\n",
      "\n",
      "For as much as government can do and must do, it is ultimately the faith and determination of the American people upon which this nation relies. It is the kindness to take in a stranger when the levees break, the selflessness of workers who would rather cut their hours than see a friend lose their job which sees us through our darkest hours. It is the firefighter's courage to storm a stairway filled with smoke, but also a parent's willingness to nurture a child, that finally decides our fate.\n",
      "\n",
      " Our challenges may be new. The instruments with which we meet them may be new. But those values upon which our success depends -- honesty and hard work, courage and fair play, tolerance and curiosity, loyalty and patriotism -- these things are old. These things are true. They have been the quiet force of progress throughout our history. What is demanded then is a return to these truths. What is required of us now is a new era of responsibility -- a recognition, on the part of every American, that we have duties to ourselves, our nation, and the world, duties that we do not grudgingly accept but rather seize gladly, firm in the knowledge that there is nothing so satisfying to the spirit, so defining of our character, than giving our all to a difficult task.\n",
      "\n",
      "This is the price and the promise of citizenship.\n",
      "\n",
      "This is the source of our confidence -- the knowledge that God calls on us to shape an uncertain destiny.\n",
      "\n",
      "This is the meaning of our liberty and our creed -- why men and women and children of every race and every faith can join in celebration across this magnificent mall, and why a man whose father less than sixty years ago might not have been served at a local restaurant can now stand before you to take a most sacred oath.\n",
      "\n",
      "So let us mark this day with remembrance, of who we are and how far we have traveled. In the year of America's birth, in the coldest of months, a small band of patriots huddled by dying campfires on the shores of an icy river. The capital was abandoned. The enemy was advancing. The snow was stained with blood. At a moment when the outcome of our revolution was most in doubt, the father of our nation ordered these words be read to the people:\n",
      "\n",
      "\"Let it be told to the future world ... that in the depth of winter, when nothing but hope and virtue could survive ... that the city and the country, alarmed at one common danger, came forth to meet ... it.\"\n",
      "\n",
      "America! In the face of our common dangers, in this winter of our hardship, let us remember these timeless words. With hope and virtue, let us brave once more the icy currents, and endure what storms may come. Let it be said by our children's children that when we were tested we refused to let this journey end, that we did not turn back nor did we falter; and with eyes fixed on the horizon and God's grace upon us, we carried forth that great gift of freedom and delivered it safely to future generations.\n",
      "\n",
      "Thank you. God bless you. And God bless the United States of America.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inaugural.raw('2009-Obama.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'fellow', 'citizens', ':', 'I', 'stand', 'here', ...]\n"
     ]
    }
   ],
   "source": [
    "print(inaugural.words('2009-Obama.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['My', 'fellow', 'citizens', ':'], ['I', 'stand', 'here', 'today', 'humbled', 'by', 'the', 'task', 'before', 'us', ',', 'grateful', 'for', 'the', 'trust', 'you', 'have', 'bestowed', ',', 'mindful', 'of', 'the', 'sacrifices', 'borne', 'by', 'our', 'ancestors', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "print(inaugural.sents('2009-Obama.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "implement function ``count_stats`` that for a given inaugural speech finds:\n",
    "* total number of words (tokens)\n",
    "* total number of distinct words (word types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_stats(doc_name):\n",
    "    '''\n",
    "    type doc_name: string\n",
    "    param doc_name: Inaugural speech name \n",
    "    rtype1: int\n",
    "    return1: total number of words (tokens)\n",
    "    rtype2: int\n",
    "    return2: totoal number of distinct words (word types)\n",
    "    '''\n",
    "    # Use the plaintext corpus reader to access a pre-tokenised list of words\n",
    "    # for the document specified in \"doc_name\"\n",
    "    doc_words = inaugural.words(doc_name)\n",
    "    # Find the total number of words in the speech\n",
    "    total_words = \n",
    "    # Find the total number of DISTINCT words in the speech\n",
    "    total_distinct_words = \n",
    "    # Return the word counts\n",
    "    return total_words, total_distinct_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your solution, evaluate the count statistics for Obama inaugural speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_name = '2009-Obama.txt'\n",
    "tokens,types = count_stats(speech_name)\n",
    "print('Total words in {}: {}'.format(speech_name, tokens))\n",
    "print('Total distinct words in {}: {}'.format(speech_name, types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Create a function ``average_stats`` to find the average word-type length of the inaugural speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_stats(doc_name):\n",
    "    '''\n",
    "    type doc_name: string\n",
    "    param doc_name: Inaugural speech name \n",
    "    rtype: float\n",
    "    return: average word type lenght per document \n",
    "    '''\n",
    "    doc_words = inaugural.words(doc_name)\n",
    "    # Construct a list that contains the word lengths for each DISTINCT word in the document\n",
    "    distinct_word_lengths = \n",
    "    # Find the average word type length\n",
    "    avg_word_length = \n",
    "    # Return the average word type length of the document\n",
    "    return avg_word_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, test your solution with Obama’s 2009 speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_name = '2009-Obama.txt'\n",
    "avg_length = average_stats(speech_name)\n",
    "print(\"Average word type length for {}: {:.3f}\".format(speech_name, avg_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Distribution\n",
    "\n",
    "A frequency distribution records the number of times each outcome of an experiment has occurred. For example, a frequency distribution could be used to record the number of times each word appears in a document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "# Obtain the words from Barack Obama’s 2009 speech\n",
    "obama_words = inaugural.words('2009-Obama.txt')\n",
    "# Construct a frequency distribution over the lowercased words in the document\n",
    "fd_obama_words = FreqDist(w.lower() for w in obama_words)\n",
    "# Find the top 50 most frequently used words in the speech\n",
    "print(fd_obama_words.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily plot the top 50 words (note `%matplotlib inline` tells jupyter that it should embed plots in the output cell after you run the code. You only need to run it once per notebook, not in every cell with a plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fd_obama_words.plot(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out how many times the words ``peace`` and ``america`` were used in the speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('peace: {}'.format(fd_obama_words['peace']))\n",
    "print('america: {}'.format(fd_obama_words['america']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Write a function ``mostFreq`` that given a name of the inaugural speech estimates the top ``k`` (default 50) most frequent words used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostFreq(doc_name, k=50):\n",
    "    '''\n",
    "    type doc_name: string\n",
    "    param doc_name: Inaugural speech name\n",
    "    type k: int (default 50)\n",
    "    param k: number of most common elements to return\n",
    "    rtype: list of tuples\n",
    "    return: list of (word, frequency) pairs\n",
    "    '''\n",
    "    doc_words = inaugural.words(doc_name)\n",
    "    # Construct a frequency distribution over the lowercased words in the document\n",
    "    fd_doc_words = \n",
    "    # Find the top x most frequently used words in the document\n",
    "    top_words = \n",
    "    # Return the top x most frequently used words\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this function to estimate the list of top 50 most frequent words of Barack Obama’s 2009 speech and\n",
    "George Washington’s 1789 speech. \n",
    "\n",
    "What can knowing word frequencies tell us about different speeches at different\n",
    "times in history?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 50 words for Obama's 2009 speech: \\n {}\".format(mostFreq('2009-Obama.txt')))\n",
    "print(\"Top 50 words for Washington's 1789 speech: \\n {}\".format(mostFreq('1789-Washington.txt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models\n",
    "\n",
    "A statistical language model assigns a probability to a sequence of words, using a probability distribution. Language models have many applications in Natural Language Processing. For example, in speech recognition, they may be used to predict the next word that a speaker will utter. In machine translation, a language model may be used to score multiple candidate translations of an input sentence to find the most fluent/natural translation from the set of candidates.\n",
    "\n",
    "In this course, to build language models we will use ``nltk_models`` package that you extracted together with this lab. It contains two classes:\n",
    "\n",
    "- ``NgramModel``: word-level ngram builder, given the desired probability estimator\n",
    "- ``LgramModel``: char-level ngram builder, given the desired probability estimator\n",
    "\n",
    "Documentation for this package can be found [here](https://tardis.ed.ac.uk/~fox/entries/nltk-model.html).\n",
    "\n",
    "Each of these classes has the following initialization:\n",
    "\n",
    "```python\n",
    "    def __init__(self, \n",
    "                 n,                    # Order of the Language model:1=unigram; 2=bigram; 3=trigram, etc.\n",
    "                 train,                # Training data (list)\n",
    "                 pad_left=False,       # Perform left padding\n",
    "                 pad_right=False,      # Perform right padding\n",
    "                 estimator=None,       # Probability distribution estimator (may or may not be smoothed)\n",
    "                 *estimator_args,      # Optional arguments for estimator\n",
    "                 **estimator_kwargs): \n",
    "```\n",
    "To import the classes execute the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from nltk_model import *  # See the README inside the nltk_model folder for more information\n",
    "except ImportError:\n",
    "    from .nltk_model import * # Compatibility depending on how this script was run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Create a function ``estimateLM`` that estimates a simple a language model using particular document of the Gutenberg corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateLM(doc_name, n):\n",
    "    '''\n",
    "    :type doc_name: string\n",
    "    :param doc_name : name of the document in gutenberg corpus.\n",
    "    :type n: int\n",
    "    :param n: order of the ngram to be estimated.\n",
    "    :rtype: NgramModel:\n",
    "    :return: language model, estimated by nltk.WittenBellProbDist estimator (default)\n",
    "    '''\n",
    "    # Construct a list of lowercase words from the document\n",
    "    words = [w.lower() for w in gutenberg.words(doc_name)]\n",
    "    lm = NgramModel(<order>,<training_data>)\n",
    "    \n",
    "    return lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function by creating a language model for a novel Sense and Sensibility by Jane Austen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = estimateLM('austen-sense.txt', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Probabilities\n",
    "\n",
    "Using the language model, we can work out the probability of a word given its context. In the case of the bigram language model build in Exercise 4, we can use a ``prob`` method of ``NgramModel`` which takes the following arguments:\n",
    "\n",
    "- ``word``: word to which the probability (score) you want to estimate\n",
    "- ``context``: a list of words that occurred before, required for ngram estimation. In the case of the bigram context is a list containing just the previous word.\n",
    "\n",
    "### Exercise 5\n",
    "\n",
    "Using the bigram language model build in Exercise 4, compute the following probabilities:\n",
    "\n",
    "1. ``reason`` followed by ``for``\n",
    "2. ``the`` followed by ``end``\n",
    "3. ``end`` followed by ``the``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Probability of 'reason' followed by 'for': {:.5f}\".format(lm.prob(word=<word>, context=[<context>])))\n",
    "print(\"Probability of 'the' followed by 'end': {:.5f}\".format(lm.prob(word=<word>, context=[<context>])))\n",
    "print(\"Probability of 'end' followed by 'the': {:.5f}\".format(lm.prob(word=<word>, context=[<context>])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Further\n",
    "\n",
    "### Smoothing\n",
    "\n",
    "Try using an estimator which does do smoothing, and see what happens to all three of the bigram probabilities. Try `help(NgramModel)` for help with the operation of this class and how to supply estimators.\n",
    "\n",
    "### Padding\n",
    "\n",
    "So far you’ve treated the data as a flat list of ‘words’, which doesn’t fully address the place of words within sentences. Using `gutenberg.sents(...)` explore the impact of the `pad left` and `pad right` argument to `NgramModel` by further editing `estimateLM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm.prob(word='The', context=['<s>']))\n",
    "print(lm.prob(word='the', context=['<s>']))\n",
    "print(lm.prob(word='End', context=['<s/>']))\n",
    "print(lm.prob(word='end', context=['<s/>']))\n",
    "print(lm.prob(word='.', context=['<s/>']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Cost vs. probabilities\n",
    "\n",
    "Redo the previous two sub-sections using *costs* instead of probabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
